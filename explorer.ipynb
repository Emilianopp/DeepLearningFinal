{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/emilianopenaloza/Git/TwitterControl/DyGLib/processed_data/CanParl/ml_CanParl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/Users/emilianopenaloza/Git/TwitterControl/DyGLib/processed_data/CanParl/ml_CanParl.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/emilianopenaloza/Git/TwitterControl/DyGLib/processed_data/CanParl/ml_CanParl.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/emilianopenaloza/Git/TwitterControl/DyGLib/processed_data/CanParl/ml_CanParl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157766400.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ts.unique()[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from statistics import mean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLock_model():\n",
    "    def __init__(self,n_nodes, n_communities, cov_dictionary , means_dictionary , num_feats) -> None:\n",
    "\n",
    "        self.arr = np.zeros((n_nodes,num_feats))\n",
    "        self.n_nodes = n_nodes\n",
    "        self.nodes_per_community = int(np.floor(n_nodes/ n_communities))\n",
    "        cur_nodes = 0\n",
    "        for c in range(n_communities):\n",
    "            x= np.random.multivariate_normal(means_dictionary[c], cov_dictionary[c], self.nodes_per_community )\n",
    "\n",
    "            self.arr[cur_nodes:cur_nodes + self.nodes_per_community,:] = x\n",
    "            cur_nodes += self.nodes_per_community\n",
    "\n",
    "        self.labels = {i: (lambda x: 0 if x> self.nodes_per_community  else 1)(i) for i in range(n_nodes)}\n",
    "        self.colors = {i: (lambda x: 'red' if x> self.nodes_per_community  else 'blue')(i) for i in range(self.n_nodes)}\n",
    "        self.G =nx.Graph()\n",
    "        nx.set_node_attributes(self.G, dict(zip(self.G.nodes(), self.arr)), 'features')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 'red' if x> self.nodes_per_community else 'blue')(i) for i in range(self.n_nodes)}, 'color')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 0 if x> self.nodes_per_community else 1)(i) for i in range(self.n_nodes)}, 'label')\n",
    "\n",
    "    \n",
    "    def get_distances(self):\n",
    "        distances = np.zeros((self.arr.shape[0],self.arr.shape[0]))\n",
    "        for i in range(self.arr.shape[0]): \n",
    "            for j in range(self.arr.shape[0]):\n",
    "                u = self.arr[i,: ]\n",
    "                v = self.arr[j,:]\n",
    "                distance = np.linalg.norm(u - v)\n",
    "                distances[i,j] = distance\n",
    "        return (distances - np.min(distances))/(np.max(distances) - np.min(distances))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def generate_Graph(self,thresh = .1):\n",
    "        G= nx.Graph()\n",
    "        distances = self.get_distances()\n",
    "        def add_edge(i,j): \n",
    "            self.G.add_edge(i,j) \n",
    "            self.G.add_edge(j,i)\n",
    "        for i in range(self.arr.shape[0]): \n",
    "            for j in range(self.arr.shape[0]):\n",
    "                if i ==j:\n",
    "                    continue\n",
    "                distance = distances[i,j]\n",
    "                r = np.random.uniform(0,1)\n",
    "                if distance < thresh: # if the covariates are close it influences the probability of forming an edge\n",
    "                    if self.labels[i] != self.labels[j]: # not in same community\n",
    "                        if r > 1:\n",
    "                            add_edge(i,j)\n",
    "                    else: # in the same community\n",
    "                        if r > 0: \n",
    "                            add_edge(i,j)\n",
    "                else:  # if covariates are further apart this decreases the probability of not forming an edge\n",
    "\n",
    "                    if self.labels[i] != self.labels[j]: # not in same community\n",
    "                        if  r> 1:\n",
    "                            add_edge(i,j)\n",
    "                    else: # in the same community\n",
    "                        if r > 0: \n",
    "                            add_edge(i,j)\n",
    "        nx.set_node_attributes(self.G, dict(zip(self.G.nodes(), self.arr)), 'features')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 'red' if x> self.nodes_per_community else 'blue')(i) for i in range(self.n_nodes)}, 'color')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 0 if x> self.nodes_per_community else 1)(i) for i in range(self.n_nodes)}, 'label')\n",
    "        \n",
    "    def set_node_features(self):\n",
    "        nx.set_node_attributes(self.G, dict(zip(self.G.nodes(), self.arr)), 'features')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 'red' if x> self.nodes_per_community else 'blue')(i) for i in range(self.n_nodes)}, 'color')\n",
    "        nx.set_node_attributes(self.G, {i: (lambda x: 0 if x> self.nodes_per_community else 1)(i) for i in range(self.n_nodes)}, 'label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def get_edges_and_attributes(self,ts):\n",
    "        edge_list = nx.to_pandas_edgelist(self.G)\n",
    "        attr = pd.DataFrame.from_dict(dict(self.G.nodes(data=True)), orient='index')\n",
    "\n",
    "        for j in range(attr.iloc[:,0].shape[0]):\n",
    "            # print(f\"==>> {attr.iloc[:,j]=} \\n {attr=}\")\n",
    "            attr[f'x{j}'] = pd.Series([x[0] for x in attr.iloc[:,0]])\n",
    "\n",
    "        attr = attr.drop(columns = ['features'])\n",
    "        edge_list['ts'] = pd.Series([ts for k in range(len(edge_list))])\n",
    "        \n",
    "        return edge_list,attr[['x1','x2']].to_numpy()\n",
    "    def draw(self):\n",
    "        nx.draw(self.G, pos = nx.spring_layout(self.G), with_labels=True, node_color = [x for x in self.colors.values()])\n",
    "        plt.show()\n",
    "\n",
    "    def hist(self):\n",
    "        # compute the degree distribution\n",
    "        degree_sequence = sorted([d for n, d in self.G.degree()], reverse=True)\n",
    "        degree_counts = nx.degree_histogram(self.G)\n",
    "\n",
    "        # plot the histogram\n",
    "        plt.bar(range(len(degree_counts)), degree_counts)\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Node Degree Histogram')\n",
    "        plt.show()\n",
    "                \n",
    "\n",
    "\n",
    "# mean = [-5, -10]\n",
    "# cov = [[1, .5], [.5, 2]]\n",
    "\n",
    "\n",
    "# mean2 = [5, 10]\n",
    "# cov2 = [[1, 3], [3, 7]]\n",
    " \n",
    "mean = [1]\n",
    "cov = [[0]]\n",
    "mean2 = [0]\n",
    "cov2 = [[0]]\n",
    " \n",
    "means = [mean, mean2]\n",
    "covs = [cov,cov2]\n",
    "\n",
    "means_dictionary = {i:x for i,x in enumerate(means)}\n",
    "cov_dictionary = {i:x for i,x in enumerate(covs)}\n",
    "\n",
    "block = BLock_model(10,2 ,cov_dictionary,means_dictionary, 1)\n",
    "\n",
    "block.generate_Graph()\n",
    "# block.get_edges_and_attributes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.get_distances()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.G.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4429/1531792098.py:9: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  x= np.random.multivariate_normal(means_dictionary[c], cov_dictionary[c], self.nodes_per_community )\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def modify_edges(G, p1= 1, p2 = 0,p3 = .8 ,threshold=.1):\n",
    "    mean = [2, 5]\n",
    "    cov = [[1, 0], [1, 0]]\n",
    "    mean2 = [0, 4]\n",
    "    cov2 = [[1, 0], [0, 1]]\n",
    "    means = [mean, mean2]\n",
    "    covs = [cov,cov2]\n",
    "    means_dictionary = {i:x for i,x in enumerate(means)}\n",
    "    cov_dictionary = {i:x for i,x in enumerate(covs)}\n",
    "    new_block = BLock_model(100,2 ,cov_dictionary,means_dictionary, 2)\n",
    "\n",
    "    new_distnaces = new_block.get_distances()\n",
    "\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u == v:\n",
    "                continue\n",
    "            # print(u)\n",
    "            # Check if u and v are in the same community\n",
    "            if G.nodes[u]['label'] == G.nodes[v]['label']:\n",
    "                # Move forward edges with probability p1\n",
    "                if random.random() < p1:\n",
    "                    new_block.G.add_edge(u, v)\n",
    "                \n",
    "            else:\n",
    "                # Move forward edges with probability p2\n",
    "                if random.random() < p2:\n",
    "                    new_block.G.add_edge(u, v)\n",
    "                \n",
    "            # Check if u and v do not have an edge and their current feature is below a certain value\n",
    "            if not G.has_edge(u, v) and new_distnaces[u,v] < threshold :\n",
    "                if random.random() < p3:\n",
    "                    new_block.G.add_edge(u, v)\n",
    "\n",
    "            # # Check if u and v have an edge and their embeddings have changed above a certain threshold, delete the edge\n",
    "            # if G.has_edge(u, v) and abs(G.nodes[u]['embedding'] - G.nodes[v]['embedding']) > threshold:\n",
    "            #     G.remove_edge(u, v)\n",
    "    # print(new_block.G.nodes)\n",
    "    return new_block\n",
    "new_block = modify_edges(block.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1], 1: [0]}\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mean = [1]\n",
    "cov = [[0]]\n",
    "mean2 = [0]\n",
    "cov2 = [[0]]\n",
    "# mean2 = [0, 4]\n",
    "# cov2 = [[1, 0], [0, 1]]\n",
    "\n",
    "means = [mean, mean2]\n",
    "covs = [cov,cov2]\n",
    "\n",
    "means_dictionary = {i:x for i,x in enumerate(means)}\n",
    "cov_dictionary = {i:x for i,x in enumerate(covs)}\n",
    "print(means_dictionary)\n",
    "\n",
    "edges_all = pd.DataFrame()\n",
    "attr_all = []\n",
    "num_nodes = 100\n",
    "base_block = BLock_model(num_nodes,2 ,cov_dictionary,means_dictionary, 1)\n",
    "base_block.generate_Graph()\n",
    "prev_edges,previous = base_block.get_edges_and_attributes(0)\n",
    "edge_lists = []\n",
    "for i in range(10):\n",
    "    # block.generate_Graph()\n",
    "    # new_block =  modify_edges(base_block.G,1,1,0)\n",
    "    # new_block.set_node_features()\n",
    "    # print(new_block.G.nodes)\n",
    "\n",
    "\n",
    "    edges,attr = base_block.get_edges_and_attributes(i)\n",
    "    assert np.array_equal(previous,attr), f'not equal, {i} , {attr=} {previous= }'\n",
    "    assert edges[['source','target']].equals(prev_edges[['source','target']]), f'{edges= } {prev_edges=}'\n",
    "    previous = attr\n",
    "    prev_edges = edges\n",
    "    print(attr.shape)\n",
    "    edge_lists.append(edges)\n",
    "    if i == 0:\n",
    "        attr_all = attr\n",
    "    else:\n",
    "        attr_all = np.vstack([attr_all,attr])\n",
    "edges_all = pd.concat(edge_lists,axis = 0)\n",
    "    # base_block = new_block\n",
    "print(attr_all.shape)\n",
    "attr_all = attr_all.reshape((10,num_nodes,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/home/emiliano/projects/def-cbravo/emiliano/DyGLib/processed_data/Synthetic/'\n",
    "edges_all= edges_all.rename(columns = {'source': 'u','target':'i'})\n",
    "edges_all['idx'] = pd.Series([ x for x in range(len(edges_all))])\n",
    "edges_all['label'] = pd.Series([ 0 for x in range(len(edges_all))])\n",
    "\n",
    "edges_all.to_csv(f'{base}/ml_Synthetic.csv',index=False)\n",
    "np.save(f'{base}/ml_Synthetic_node.npy',attr_all[0])\n",
    "edge_attr = np.ones(len(edges_all)).reshape(-1,1)\n",
    "np.save(f'{base}/ml_Synthetic.npy',edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>ts</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63475</th>\n",
       "      <td>475</td>\n",
       "      <td>487</td>\n",
       "      <td>9</td>\n",
       "      <td>63475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63476</th>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "      <td>9</td>\n",
       "      <td>63476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63477</th>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>9</td>\n",
       "      <td>63477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63478</th>\n",
       "      <td>478</td>\n",
       "      <td>496</td>\n",
       "      <td>9</td>\n",
       "      <td>63478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63479</th>\n",
       "      <td>487</td>\n",
       "      <td>496</td>\n",
       "      <td>9</td>\n",
       "      <td>63479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>634800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u    i  ts    idx  label\n",
       "0        0    1   0      0      0\n",
       "1        0    2   0      1      0\n",
       "2        0    3   0      2      0\n",
       "3        0    4   0      3      0\n",
       "4        0    5   0      4      0\n",
       "...    ...  ...  ..    ...    ...\n",
       "63475  475  487   9  63475      0\n",
       "63476  475  496   9  63476      0\n",
       "63477  478  487   9  63477      0\n",
       "63478  478  496   9  63478      0\n",
       "63479  487  496   9  63479      0\n",
       "\n",
       "[634800 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2ab01a5ed9a0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "l = []\n",
    "for u in edges_all.u.unique():\n",
    "    cur_df = edges_all[edges_all.u == u]\n",
    "    if u >250:\n",
    "        prop = cur_df.i.apply(lambda x: 1 if u>250 and x>250 else 0)\n",
    "    else:\n",
    "        prop = cur_df.i.apply(lambda x: 1 if u<250 and x<250 else 0)\n",
    "    l.append(prop.sum()/len(prop))\n",
    "sns.displot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m node_id \u001b[39m=\u001b[39m source_nodes[source_index]\n\u001b[1;32m     12\u001b[0m interact_time \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m---> 13\u001b[0m i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msearchsorted(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mnodes_neighbor_times[node_id], interact_time \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m neighbors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes_neighbor_ids[node_id][:i]\n\u001b[1;32m     16\u001b[0m \u001b[39m# remove the source node from the neighbors array\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assume these are the source and destination nodes\n",
    "source_nodes = np.array([1, 2, 3, 4, 5])\n",
    "dest_nodes = np.array([2, 4, 1, 3, 5])\n",
    "\n",
    "# assume this is the index of the source node that you want to remove from the destination nodes array\n",
    "source_index = 2\n",
    "\n",
    "# find the neighbors of the source node before a certain timestamp\n",
    "node_id = source_nodes[source_index]\n",
    "interact_time = 5\n",
    "i = np.searchsorted(self.nodes_neighbor_times[node_id], interact_time + 1)\n",
    "neighbors = self.nodes_neighbor_ids[node_id][:i]\n",
    "\n",
    "# remove the source node from the neighbors array\n",
    "index_to_remove = np.where(neighbors == node_id)[0]\n",
    "if len(index_to_remove) > 0:\n",
    "    neighbors = np.delete(neighbors, index_to_remove[0])\n",
    "\n",
    "# remove the source node from the destination nodes array\n",
    "dest_nodes = np.delete(dest_nodes, source_index)\n",
    "\n",
    "print(neighbors)\n",
    "print(dest_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('/Users/emilianopenaloza/Git/TwitterControl/DyGLib/processed_data/CanParl/ml_CanParl.csv')\n",
    "s = set(d.u) | set(d.i)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def make_data(df):\n",
    "\n",
    "    src_node_ids = df.u.values.astype(np.long)\n",
    "    dst_node_ids = df.i.values.astype(np.long)\n",
    "    node_interact_times = df.ts.values.astype(np.float64)\n",
    "    labels = df.label.values\n",
    "    \n",
    "    edge_ids = np.array(list(range(len(df.idx.values)))).astype(np.long)\n",
    "    labels = df.label.values\n",
    "\n",
    "    formatted_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids,\n",
    "                      node_interact_times=node_interact_times,\n",
    "                    edge_ids=edge_ids, labels=labels)\n",
    "    \n",
    "    return formatted_data\n",
    "    \n",
    "\n",
    "\n",
    "def get_dataset_subset(graph_df,percentage):\n",
    "\n",
    "    sampled = graph_df.groupby('ts', group_keys=False).apply(lambda x: x.sample(frac=percentage))\n",
    "\n",
    "    graph_df =  graph_df[~graph_df.isin(sampled)].dropna()\n",
    "   \n",
    "    return sampled,graph_df\n",
    "\n",
    "\n",
    "def make_data_dictionaries(graph_dfs , d, edge_raw_features,node_raw_features ,time_varying_features):\n",
    "    prev_edges = 0\n",
    "\n",
    "    NODE_FEAT_DIM = EDGE_FEAT_DIM = 172\n",
    "    for ts,graph_df in enumerate(graph_dfs.items()):\n",
    "        \n",
    "        graph_df = graph_df[1]\n",
    "\n",
    "        new_df = pd.DataFrame(edge_raw_features[prev_edges:prev_edges + len(graph_df)])\n",
    "\n",
    "        new_df.reset_index(drop=True, inplace=True)\n",
    "        graph_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        graph_df = pd.concat([graph_df, new_df], axis=1)\n",
    "        print(graph_df.ts.unique())\n",
    "    \n",
    "        if not time_varying_features:\n",
    "            if node_raw_features.shape[1 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features.shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_features[ts] = np.concatenate([node_raw_features, node_zero_padding], axis=1)\n",
    "            \n",
    "        else:\n",
    "            if node_raw_features.shape[2 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features[ts].shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_raw_features = np.concatenate([node_raw_features[ts], node_zero_padding], axis=1)\n",
    "\n",
    "        if edge_raw_features.shape[1] < EDGE_FEAT_DIM:\n",
    "                edge_zero_padding = np.zeros((edge_raw_features.shape[0], 172 - edge_raw_features.shape[1]))\n",
    "                edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)\n",
    "\n",
    "        assert NODE_FEAT_DIM == node_raw_features.shape[1] and EDGE_FEAT_DIM == edge_raw_features.shape[1], \"Unaligned feature dimensions after feature padding!\"\n",
    "   \n",
    "        d[ts]= {'edges': make_data(graph_df), 'node_features' : node_raw_features , 'edge_features':edge_raw_features}\n",
    "        prev_edges += len(graph_df)\n",
    "\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def get_link_prediction_data_snapshots(dataset_name: str, val_ratio: float, test_ratio: float):\n",
    "    \"\"\"\n",
    "    generate data for link prediction task (inductive & transductive settings)\n",
    "    :param dataset_name: str, dataset name\n",
    "    :param val_ratio: float, validation data ratio\n",
    "    :param test_ratio: float, test data ratio\n",
    "    :return: node_raw_features, edge_raw_features, (np.ndarray),\n",
    "            full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data, (Data object)\n",
    "    \"\"\"\n",
    "    # Load data and train val test split\n",
    "    graph_df = pd.read_csv('./processed_data/{}/ml_{}.csv'.format(dataset_name, dataset_name))\n",
    "    \n",
    "    edge_raw_features = np.load('./processed_data/{}/ml_{}.npy'.format(dataset_name, dataset_name))\n",
    "    node_raw_features = np.load('./processed_data/{}/ml_{}_node.npy'.format(dataset_name, dataset_name))\n",
    "    cur_edges = 0\n",
    "    print(f\"==>> {graph_df.ts.unique()=}\")\n",
    "    full_data_unstacked = make_data(graph_df)\n",
    "    if len(node_raw_features.shape) > 2: \n",
    "        time_varying_features = True\n",
    "    else: \n",
    "        time_varying_features = False\n",
    "    val_graph_df , train_graph_df = get_dataset_subset(graph_df, val_ratio)\n",
    "    print(f\"==>> {val_graph_df.ts.unique()=}\")\n",
    "\n",
    "    print(train_graph_df.ts.unique())\n",
    "\n",
    "    test_graph_df , train_graph_df = get_dataset_subset(train_graph_df, (len(graph_df) * test_ratio ) /len(train_graph_df))\n",
    "    train_data_unstacked = make_data(train_graph_df)\n",
    "\n",
    "\n",
    "    train_graph_dfs = dict(tuple(train_graph_df.groupby('ts')))\n",
    "    val_graph_dfs = dict(tuple(val_graph_df.groupby('ts')))\n",
    "    test_graph_dfs = dict(tuple(test_graph_df.groupby('ts')))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "    train_data = make_data_dictionaries(train_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    val_data = make_data_dictionaries(val_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    test_data  =make_data_dictionaries(test_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "\n",
    "    return full_data_unstacked,train_data_unstacked,train_data,val_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> graph_df.ts.unique()=array([0.000000e+00, 3.153600e+07, 6.307200e+07, 9.469440e+07,\n",
      "       1.262304e+08, 1.577664e+08, 1.893024e+08, 2.209248e+08,\n",
      "       2.524608e+08, 2.839968e+08, 3.155328e+08, 3.471552e+08,\n",
      "       3.786912e+08, 4.102272e+08])\n",
      "ts                \n",
      "0.0          2281             0.0\n",
      "             668              0.0\n",
      "             3591             0.0\n",
      "             11               0.0\n",
      "             3135             0.0\n",
      "                         ...     \n",
      "410227200.0  73743    410227200.0\n",
      "             72675    410227200.0\n",
      "             72380    410227200.0\n",
      "             71592    410227200.0\n",
      "             72938    410227200.0\n",
      "Name: ts, Length: 140, dtype: float64\n",
      "==>> percentage=0.1\n",
      "==>> graph_df.ts.unique()=array([0.000000e+00, 3.153600e+07, 6.307200e+07, 9.469440e+07,\n",
      "       1.262304e+08, 1.577664e+08, 1.893024e+08, 2.209248e+08,\n",
      "       2.524608e+08, 2.839968e+08, 3.155328e+08, 3.471552e+08,\n",
      "       3.786912e+08, 4.102272e+08])\n",
      "==>> sampled.ts.unique()=array([0.000000e+00, 3.153600e+07, 6.307200e+07, 9.469440e+07,\n",
      "       1.262304e+08, 1.577664e+08, 1.893024e+08, 2.209248e+08,\n",
      "       2.524608e+08, 2.839968e+08, 3.155328e+08, 3.471552e+08,\n",
      "       3.786912e+08, 4.102272e+08])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m full_data_unstacked,train_data_unstacked,train_datas,val_datas,test_datas\u001b[39m=\u001b[39m \\\n\u001b[0;32m----> 3\u001b[0m     get_link_prediction_data_snapshots(dataset_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCanParl\u001b[39;49m\u001b[39m'\u001b[39;49m, val_ratio\u001b[39m=\u001b[39;49m\u001b[39m.1\u001b[39;49m, test_ratio\u001b[39m=\u001b[39;49m\u001b[39m.2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[31], line 110\u001b[0m, in \u001b[0;36mget_link_prediction_data_snapshots\u001b[0;34m(dataset_name, val_ratio, test_ratio)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    109\u001b[0m     time_varying_features \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m val_graph_df , train_graph_df \u001b[39m=\u001b[39m get_dataset_subset(graph_df, val_ratio)\n\u001b[1;32m    111\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==>> \u001b[39m\u001b[39m{\u001b[39;00mval_graph_df\u001b[39m.\u001b[39mts\u001b[39m.\u001b[39munique()\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(train_graph_df\u001b[39m.\u001b[39mts\u001b[39m.\u001b[39munique())\n",
      "Cell \u001b[0;32mIn[31], line 45\u001b[0m, in \u001b[0;36mget_dataset_subset\u001b[0;34m(graph_df, percentage)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==>> \u001b[39m\u001b[39m{\u001b[39;00mgraph_df\u001b[39m.\u001b[39mts\u001b[39m.\u001b[39munique()\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==>> \u001b[39m\u001b[39m{\u001b[39;00msampled\u001b[39m.\u001b[39mts\u001b[39m.\u001b[39munique()\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m sampled,graph_df\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "full_data_unstacked,train_data_unstacked,train_datas,val_datas,test_datas= \\\n",
    "    get_link_prediction_data_snapshots(dataset_name='CanParl', val_ratio=.1, test_ratio=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "from models.modules import TimeEncoder\n",
    "from utils.utils import NeighborSampler\n",
    "\n",
    "\n",
    "class DyGFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, node_raw_features: np.ndarray, edge_raw_features: np.ndarray, neighbor_sampler: NeighborSampler,\n",
    "                 time_feat_dim: int, channel_embedding_dim: int, patch_size: int = 1, num_layers: int = 2, num_heads: int = 2,\n",
    "                 dropout: float = 0.1, max_input_sequence_length: int = 512, device: str = 'cpu',node_feature_dim= None,edge_feature_dim= None ):\n",
    "        \"\"\"\n",
    "        DyGFormer model.\n",
    "        :param node_raw_features: ndarray, shape (num_nodes + 1, node_feat_dim)\n",
    "        :param edge_raw_features: ndarray, shape (num_edges + 1, edge_feat_dim)\n",
    "        :param neighbor_sampler: neighbor sampler\n",
    "        :param time_feat_dim: int, dimension of time features (encodings)\n",
    "        :param channel_embedding_dim: int, dimension of each channel embedding\n",
    "        :param patch_size: int, patch size\n",
    "        :param num_layers: int, number of transformer layers\n",
    "        :param num_heads: int, number of attention heads\n",
    "        :param dropout: float, dropout rate\n",
    "        :param max_input_sequence_length: int, maximal length of the input sequence for each node\n",
    "        :param device: str, device\n",
    "        \"\"\"\n",
    "        super(DyGFormer, self).__init__()\n",
    "        self.device = device\n",
    "        if edge_raw_features and node_raw_features:\n",
    "            self.node_raw_features = torch.from_numpy(node_raw_features.astype(np.float32)).to(device)\n",
    "            self.edge_raw_features = torch.from_numpy(edge_raw_features.astype(np.float32)).to(device)\n",
    "            self.node_feat_dim = self.node_raw_features.shape[1]\n",
    "            self.edge_feat_dim = self.edge_raw_features.shape[1]\n",
    "        if node_feature_dim and edge_feature_dim:\n",
    "            self.node_feat_dim = node_feature_dim \n",
    "            self.edge_feat_dim = edge_feature_dim\n",
    "\n",
    "\n",
    "        self.neighbor_sampler = neighbor_sampler\n",
    "        self.time_feat_dim = time_feat_dim\n",
    "        self.channel_embedding_dim = channel_embedding_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.max_input_sequence_length = max_input_sequence_length\n",
    "        self.device = device\n",
    "\n",
    "        self.time_encoder = TimeEncoder(time_dim=time_feat_dim)\n",
    "\n",
    "        self.neighbor_co_occurrence_feat_dim = self.channel_embedding_dim\n",
    "        self.neighbor_co_occurrence_encoder = NeighborCooccurrenceEncoder(neighbor_co_occurrence_feat_dim=self.neighbor_co_occurrence_feat_dim, device=self.device)\n",
    "\n",
    "        self.projection_layer = nn.ModuleDict({\n",
    "            'node': nn.Linear(in_features=self.patch_size * self.node_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'edge': nn.Linear(in_features=self.patch_size * self.edge_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'time': nn.Linear(in_features=self.patch_size * self.time_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'neighbor_co_occurrence': nn.Linear(in_features=self.patch_size * self.neighbor_co_occurrence_feat_dim, out_features=self.channel_embedding_dim, bias=True)\n",
    "        })\n",
    "\n",
    "        self.num_channels = 4\n",
    "\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerEncoder(attention_dim=self.num_channels * self.channel_embedding_dim, num_heads=self.num_heads, dropout=self.dropout)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.num_channels * self.channel_embedding_dim, out_features=self.node_feat_dim, bias=True)\n",
    "    def set_edge_node_features(self,node_raw_features,edge_raw_features):\n",
    "        self.node_raw_features = torch.from_numpy(node_raw_features.astype(np.float32)).to(self.device)\n",
    "        self.edge_raw_features = torch.from_numpy(edge_raw_features.astype(np.float32)).to(self.device)\n",
    "      \n",
    "    \n",
    "\n",
    "         \n",
    "\n",
    "    def compute_src_dst_node_temporal_embeddings(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray):\n",
    "        \"\"\"\n",
    "        compute source and destination node temporal embeddings\n",
    "        :param src_node_ids: ndarray, shape (batch_size, )\n",
    "        :param dst_node_ids: ndarray, shape (batch_size, )\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get the first-hop neighbors of source and destination nodes\n",
    "        # three lists to store source nodes' first-hop neighbor ids, edge ids and interaction timestamp information, with batch_size as the list length\n",
    "        # print(src_node_ids, node_interact_times)\n",
    "        src_nodes_neighbor_ids_list, src_nodes_edge_ids_list, src_nodes_neighbor_times_list = \\\n",
    "            self.neighbor_sampler.get_all_first_hop_neighbors(node_ids=src_node_ids, node_interact_times=node_interact_times)\n",
    "\n",
    "        # three lists to store destination nodes' first-hop neighbor ids, edge ids and interaction timestamp information, with batch_size as the list length\n",
    "        dst_nodes_neighbor_ids_list, dst_nodes_edge_ids_list, dst_nodes_neighbor_times_list = \\\n",
    "            self.neighbor_sampler.get_all_first_hop_neighbors(node_ids=dst_node_ids, node_interact_times=node_interact_times)\n",
    "\n",
    "        # pad the sequences of first-hop neighbors for source and destination nodes\n",
    "        # src_padded_nodes_neighbor_ids, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        # src_padded_nodes_edge_ids, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        # src_padded_nodes_neighbor_times, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        src_padded_nodes_neighbor_ids, src_padded_nodes_edge_ids, src_padded_nodes_neighbor_times = \\\n",
    "            self.pad_sequences(node_ids=src_node_ids, node_interact_times=node_interact_times, nodes_neighbor_ids_list=src_nodes_neighbor_ids_list,\n",
    "                               nodes_edge_ids_list=src_nodes_edge_ids_list, nodes_neighbor_times_list=src_nodes_neighbor_times_list,\n",
    "                               patch_size=self.patch_size, max_input_sequence_length=self.max_input_sequence_length)\n",
    "\n",
    "        # dst_padded_nodes_neighbor_ids, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        # dst_padded_nodes_edge_ids, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        # dst_padded_nodes_neighbor_times, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        dst_padded_nodes_neighbor_ids, dst_padded_nodes_edge_ids, dst_padded_nodes_neighbor_times = \\\n",
    "            self.pad_sequences(node_ids=dst_node_ids, node_interact_times=node_interact_times, nodes_neighbor_ids_list=dst_nodes_neighbor_ids_list,\n",
    "                               nodes_edge_ids_list=dst_nodes_edge_ids_list, nodes_neighbor_times_list=dst_nodes_neighbor_times_list,\n",
    "                               patch_size=self.patch_size, max_input_sequence_length=self.max_input_sequence_length)\n",
    "\n",
    "        # src_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        src_padded_nodes_neighbor_co_occurrence_features, dst_padded_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.neighbor_co_occurrence_encoder(src_padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                                                dst_padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids)\n",
    "\n",
    "        # get the features of the sequence of source and destination nodes\n",
    "        # src_padded_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, src_max_seq_length, node_feat_dim)\n",
    "        # src_padded_nodes_edge_raw_features, Tensor, shape (batch_size, src_max_seq_length, edge_feat_dim)\n",
    "        # src_padded_nodes_neighbor_time_features, Tensor, shape (batch_size, src_max_seq_length, time_feat_dim)\n",
    "        src_padded_nodes_neighbor_node_raw_features, src_padded_nodes_edge_raw_features, src_padded_nodes_neighbor_time_features = \\\n",
    "            self.get_features(node_interact_times=node_interact_times, padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                              padded_nodes_edge_ids=src_padded_nodes_edge_ids, padded_nodes_neighbor_times=src_padded_nodes_neighbor_times, time_encoder=self.time_encoder)\n",
    "\n",
    "        # dst_padded_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, dst_max_seq_length, node_feat_dim)\n",
    "        # dst_padded_nodes_edge_raw_features, Tensor, shape (batch_size, dst_max_seq_length, edge_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_time_features, Tensor, shape (batch_size, dst_max_seq_length, time_feat_dim)\n",
    "        dst_padded_nodes_neighbor_node_raw_features, dst_padded_nodes_edge_raw_features, dst_padded_nodes_neighbor_time_features = \\\n",
    "            self.get_features(node_interact_times=node_interact_times, padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids,\n",
    "                              padded_nodes_edge_ids=dst_padded_nodes_edge_ids, padded_nodes_neighbor_times=dst_padded_nodes_neighbor_times, time_encoder=self.time_encoder)\n",
    "\n",
    "        # get the patches for source and destination nodes\n",
    "        # src_patches_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, src_num_patches, patch_size * node_feat_dim)\n",
    "        # src_patches_nodes_edge_raw_features, Tensor, shape (batch_size, src_num_patches, patch_size * edge_feat_dim)\n",
    "        # src_patches_nodes_neighbor_time_features, Tensor, shape (batch_size, src_num_patches, patch_size * time_feat_dim)\n",
    "        src_patches_nodes_neighbor_node_raw_features, src_patches_nodes_edge_raw_features, \\\n",
    "        src_patches_nodes_neighbor_time_features, src_patches_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.get_patches(padded_nodes_neighbor_node_raw_features=src_padded_nodes_neighbor_node_raw_features,\n",
    "                             padded_nodes_edge_raw_features=src_padded_nodes_edge_raw_features,\n",
    "                             padded_nodes_neighbor_time_features=src_padded_nodes_neighbor_time_features,\n",
    "                             padded_nodes_neighbor_co_occurrence_features=src_padded_nodes_neighbor_co_occurrence_features,\n",
    "                             patch_size=self.patch_size)\n",
    "\n",
    "        # dst_patches_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, dst_num_patches, patch_size * node_feat_dim)\n",
    "        # dst_patches_nodes_edge_raw_features, Tensor, shape (batch_size, dst_num_patches, patch_size * edge_feat_dim)\n",
    "        # dst_patches_nodes_neighbor_time_features, Tensor, shape (batch_size, dst_num_patches, patch_size * time_feat_dim)\n",
    "        dst_patches_nodes_neighbor_node_raw_features, dst_patches_nodes_edge_raw_features, \\\n",
    "        dst_patches_nodes_neighbor_time_features, dst_patches_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.get_patches(padded_nodes_neighbor_node_raw_features=dst_padded_nodes_neighbor_node_raw_features,\n",
    "                             padded_nodes_edge_raw_features=dst_padded_nodes_edge_raw_features,\n",
    "                             padded_nodes_neighbor_time_features=dst_padded_nodes_neighbor_time_features,\n",
    "                             padded_nodes_neighbor_co_occurrence_features=dst_padded_nodes_neighbor_co_occurrence_features,\n",
    "                             patch_size=self.patch_size)\n",
    "\n",
    "        # align the patch encoding dimension\n",
    "        # Tensor, shape (batch_size, src_num_patches, channel_embedding_dim)\n",
    "        src_patches_nodes_neighbor_node_raw_features = self.projection_layer['node'](src_patches_nodes_neighbor_node_raw_features)\n",
    "        src_patches_nodes_edge_raw_features = self.projection_layer['edge'](src_patches_nodes_edge_raw_features)\n",
    "        src_patches_nodes_neighbor_time_features = self.projection_layer['time'](src_patches_nodes_neighbor_time_features)\n",
    "        src_patches_nodes_neighbor_co_occurrence_features = self.projection_layer['neighbor_co_occurrence'](src_patches_nodes_neighbor_co_occurrence_features)\n",
    "\n",
    "        # Tensor, shape (batch_size, dst_num_patches, channel_embedding_dim)\n",
    "        dst_patches_nodes_neighbor_node_raw_features = self.projection_layer['node'](dst_patches_nodes_neighbor_node_raw_features)\n",
    "        dst_patches_nodes_edge_raw_features = self.projection_layer['edge'](dst_patches_nodes_edge_raw_features)\n",
    "        dst_patches_nodes_neighbor_time_features = self.projection_layer['time'](dst_patches_nodes_neighbor_time_features)\n",
    "        dst_patches_nodes_neighbor_co_occurrence_features = self.projection_layer['neighbor_co_occurrence'](dst_patches_nodes_neighbor_co_occurrence_features)\n",
    "\n",
    "        batch_size = len(src_patches_nodes_neighbor_node_raw_features)\n",
    "        src_num_patches = src_patches_nodes_neighbor_node_raw_features.shape[1]\n",
    "        dst_num_patches = dst_patches_nodes_neighbor_node_raw_features.shape[1]\n",
    "\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, channel_embedding_dim)\n",
    "        patches_nodes_neighbor_node_raw_features = torch.cat([src_patches_nodes_neighbor_node_raw_features, dst_patches_nodes_neighbor_node_raw_features], dim=1)\n",
    "        patches_nodes_edge_raw_features = torch.cat([src_patches_nodes_edge_raw_features, dst_patches_nodes_edge_raw_features], dim=1)\n",
    "        patches_nodes_neighbor_time_features = torch.cat([src_patches_nodes_neighbor_time_features, dst_patches_nodes_neighbor_time_features], dim=1)\n",
    "        patches_nodes_neighbor_co_occurrence_features = torch.cat([src_patches_nodes_neighbor_co_occurrence_features, dst_patches_nodes_neighbor_co_occurrence_features], dim=1)\n",
    "\n",
    "        patches_data = [patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features,\n",
    "                        patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features]\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels, channel_embedding_dim)\n",
    "        patches_data = torch.stack(patches_data, dim=2)\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        patches_data = patches_data.reshape(batch_size, src_num_patches + dst_num_patches, self.num_channels * self.channel_embedding_dim)\n",
    "\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        for transformer in self.transformers:\n",
    "            patches_data = transformer(patches_data)\n",
    "\n",
    "        # src_patches_data, Tensor, shape (batch_size, src_num_patches, num_channels * channel_embedding_dim)\n",
    "        src_patches_data = patches_data[:, : src_num_patches, :]\n",
    "        # dst_patches_data, Tensor, shape (batch_size, dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        dst_patches_data = patches_data[:, src_num_patches: src_num_patches + dst_num_patches, :]\n",
    "        # src_patches_data, Tensor, shape (batch_size, num_channels * channel_embedding_dim)\n",
    "        src_patches_data = torch.mean(src_patches_data, dim=1)\n",
    "        # dst_patches_data, Tensor, shape (batch_size, num_channels * channel_embedding_dim)\n",
    "        dst_patches_data = torch.mean(dst_patches_data, dim=1)\n",
    "\n",
    "        # Tensor, shape (batch_size, node_feat_dim)\n",
    "        src_node_embeddings = self.output_layer(src_patches_data)\n",
    "        # Tensor, shape (batch_size, node_feat_dim)\n",
    "        dst_node_embeddings = self.output_layer(dst_patches_data)\n",
    "\n",
    "        return src_node_embeddings, dst_node_embeddings\n",
    "\n",
    "    def pad_sequences(self, node_ids: np.ndarray, node_interact_times: np.ndarray, nodes_neighbor_ids_list: list, nodes_edge_ids_list: list,\n",
    "                      nodes_neighbor_times_list: list, patch_size: int = 1, max_input_sequence_length: int = 256):\n",
    "        \"\"\"\n",
    "        pad the sequences for nodes in node_ids\n",
    "        :param node_ids: ndarray, shape (batch_size, )\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :param nodes_neighbor_ids_list: list of ndarrays, each ndarray contains neighbor ids for nodes in node_ids\n",
    "        :param nodes_edge_ids_list: list of ndarrays, each ndarray contains edge ids for nodes in node_ids\n",
    "        :param nodes_neighbor_times_list: list of ndarrays, each ndarray contains neighbor interaction timestamp for nodes in node_ids\n",
    "        :param patch_size: int, patch size\n",
    "        :param max_input_sequence_length: int, maximal number of neighbors for each node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert max_input_sequence_length - 1 > 0, 'Maximal number of neighbors for each node should be greater than 1!'\n",
    "        max_seq_length = 0\n",
    "        # first cut the sequence of nodes whose number of neighbors is more than max_input_sequence_length - 1 (we need to include the target node in the sequence)\n",
    "        for idx in range(len(nodes_neighbor_ids_list)):\n",
    "            assert len(nodes_neighbor_ids_list[idx]) == len(nodes_edge_ids_list[idx]) == len(nodes_neighbor_times_list[idx])\n",
    "            if len(nodes_neighbor_ids_list[idx]) > max_input_sequence_length - 1:\n",
    "                # cut the sequence by taking the most recent max_input_sequence_length interactions\n",
    "                nodes_neighbor_ids_list[idx] = nodes_neighbor_ids_list[idx][-(max_input_sequence_length - 1):]\n",
    "                nodes_edge_ids_list[idx] = nodes_edge_ids_list[idx][-(max_input_sequence_length - 1):]\n",
    "                nodes_neighbor_times_list[idx] = nodes_neighbor_times_list[idx][-(max_input_sequence_length - 1):]\n",
    "            if len(nodes_neighbor_ids_list[idx]) > max_seq_length:\n",
    "                max_seq_length = len(nodes_neighbor_ids_list[idx])\n",
    "\n",
    "        # include the target node itself\n",
    "        max_seq_length += 1\n",
    "        if max_seq_length % patch_size != 0:\n",
    "            max_seq_length += (patch_size - max_seq_length % patch_size)\n",
    "        assert max_seq_length % patch_size  == 0\n",
    "\n",
    "        # pad the sequences\n",
    "        # three ndarrays with shape (batch_size, max_seq_length)\n",
    "        padded_nodes_neighbor_ids = np.zeros((len(node_ids), max_seq_length)).astype(np.long)\n",
    "        padded_nodes_edge_ids = np.zeros((len(node_ids), max_seq_length)).astype(np.long)\n",
    "        padded_nodes_neighbor_times = np.zeros((len(node_ids), max_seq_length)).astype(np.float32)\n",
    "\n",
    "        for idx in range(len(node_ids)):\n",
    "            padded_nodes_neighbor_ids[idx, 0] = node_ids[idx]\n",
    "            padded_nodes_edge_ids[idx, 0] = 0\n",
    "            padded_nodes_neighbor_times[idx, 0] = node_interact_times[idx]\n",
    "\n",
    "            if len(nodes_neighbor_ids_list[idx]) > 0:\n",
    "                padded_nodes_neighbor_ids[idx, 1: len(nodes_neighbor_ids_list[idx]) + 1] = nodes_neighbor_ids_list[idx]\n",
    "                padded_nodes_edge_ids[idx, 1: len(nodes_edge_ids_list[idx]) + 1] = nodes_edge_ids_list[idx]\n",
    "                padded_nodes_neighbor_times[idx, 1: len(nodes_neighbor_times_list[idx]) + 1] = nodes_neighbor_times_list[idx]\n",
    "\n",
    "        # three ndarrays with shape (batch_size, max_seq_length)\n",
    "        return padded_nodes_neighbor_ids, padded_nodes_edge_ids, padded_nodes_neighbor_times\n",
    "\n",
    "    def get_features(self, node_interact_times: np.ndarray, padded_nodes_neighbor_ids: np.ndarray, padded_nodes_edge_ids: np.ndarray,\n",
    "                     padded_nodes_neighbor_times: np.ndarray, time_encoder: TimeEncoder):\n",
    "        \"\"\"\n",
    "        get node, edge and time features\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :param padded_nodes_neighbor_ids: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param padded_nodes_edge_ids: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param padded_nodes_neighbor_times: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param time_encoder: TimeEncoder, time encoder\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Tensor, shape (batch_size, max_seq_length, node_feat_dim)\n",
    "        padded_nodes_neighbor_node_raw_features = self.node_raw_features[torch.from_numpy(padded_nodes_neighbor_ids)]\n",
    "        # Tensor, shape (batch_size, max_seq_length, edge_feat_dim)\n",
    "        padded_nodes_edge_raw_features = self.edge_raw_features[torch.from_numpy(padded_nodes_edge_ids)]\n",
    "        # Tensor, shape (batch_size, max_seq_length, time_feat_dim)\n",
    "        padded_nodes_neighbor_time_features = time_encoder(timestamps=torch.from_numpy(node_interact_times[:, np.newaxis] - padded_nodes_neighbor_times).float().to(self.device))\n",
    "\n",
    "        # ndarray, set the time features to all zeros for the padded timestamp\n",
    "        padded_nodes_neighbor_time_features[torch.from_numpy(padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "\n",
    "        return padded_nodes_neighbor_node_raw_features, padded_nodes_edge_raw_features, padded_nodes_neighbor_time_features\n",
    "\n",
    "    def get_patches(self, padded_nodes_neighbor_node_raw_features: torch.Tensor, padded_nodes_edge_raw_features: torch.Tensor,\n",
    "                    padded_nodes_neighbor_time_features: torch.Tensor, padded_nodes_neighbor_co_occurrence_features: torch.Tensor = None, patch_size: int = 1):\n",
    "        \"\"\"\n",
    "        get the sequence of patches for nodes\n",
    "        :param padded_nodes_neighbor_node_raw_features: Tensor, shape (batch_size, max_seq_length, node_feat_dim)\n",
    "        :param padded_nodes_edge_raw_features: Tensor, shape (batch_size, max_seq_length, edge_feat_dim)\n",
    "        :param padded_nodes_neighbor_time_features: Tensor, shape (batch_size, max_seq_length, time_feat_dim)\n",
    "        :param padded_nodes_neighbor_co_occurrence_features: Tensor, shape (batch_size, max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        :param patch_size: int, patch size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert padded_nodes_neighbor_node_raw_features.shape[1] % patch_size == 0\n",
    "        num_patches = padded_nodes_neighbor_node_raw_features.shape[1] // patch_size\n",
    "\n",
    "        # list of Tensors with shape (num_patches, ), each Tensor with shape (batch_size, patch_size, node_feat_dim)\n",
    "        patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features, \\\n",
    "        patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features = [], [], [], []\n",
    "\n",
    "        for patch_id in range(num_patches):\n",
    "            start_idx = patch_id * patch_size\n",
    "            end_idx = patch_id * patch_size + patch_size\n",
    "            patches_nodes_neighbor_node_raw_features.append(padded_nodes_neighbor_node_raw_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_edge_raw_features.append(padded_nodes_edge_raw_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_neighbor_time_features.append(padded_nodes_neighbor_time_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_neighbor_co_occurrence_features.append(padded_nodes_neighbor_co_occurrence_features[:, start_idx: end_idx, :])\n",
    "\n",
    "        batch_size = len(padded_nodes_neighbor_node_raw_features)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * node_feat_dim)\n",
    "        patches_nodes_neighbor_node_raw_features = torch.stack(patches_nodes_neighbor_node_raw_features, dim=1).reshape(batch_size, num_patches, patch_size * self.node_feat_dim)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * edge_feat_dim)\n",
    "        patches_nodes_edge_raw_features = torch.stack(patches_nodes_edge_raw_features, dim=1).reshape(batch_size, num_patches, patch_size * self.edge_feat_dim)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * time_feat_dim)\n",
    "        patches_nodes_neighbor_time_features = torch.stack(patches_nodes_neighbor_time_features, dim=1).reshape(batch_size, num_patches, patch_size * self.time_feat_dim)\n",
    "\n",
    "        patches_nodes_neighbor_co_occurrence_features = torch.stack(patches_nodes_neighbor_co_occurrence_features, dim=1).reshape(batch_size, num_patches, patch_size * self.neighbor_co_occurrence_feat_dim)\n",
    "\n",
    "        return patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features, patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features\n",
    "\n",
    "    def set_neighbor_sampler(self, neighbor_sampler: NeighborSampler):\n",
    "        \"\"\"\n",
    "        set neighbor sampler to neighbor_sampler and reset the random state (for reproducing the results for uniform and time_interval_aware sampling)\n",
    "        :param neighbor_sampler: NeighborSampler, neighbor sampler\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.neighbor_sampler = neighbor_sampler\n",
    "        if self.neighbor_sampler.sample_neighbor_strategy in ['uniform', 'time_interval_aware']:\n",
    "            assert self.neighbor_sampler.seed is not None\n",
    "            self.neighbor_sampler.reset_random_state()\n",
    "\n",
    "\n",
    "class NeighborCooccurrenceEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neighbor_co_occurrence_feat_dim: int, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Neighbor co-occurrence encoder.\n",
    "        :param neighbor_co_occurrence_feat_dim: int, dimension of neighbor co-occurrence features (encodings)\n",
    "        :param device: str, device\n",
    "        \"\"\"\n",
    "        super(NeighborCooccurrenceEncoder, self).__init__()\n",
    "        self.neighbor_co_occurrence_feat_dim = neighbor_co_occurrence_feat_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.neighbor_co_occurrence_encode_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=self.neighbor_co_occurrence_feat_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.neighbor_co_occurrence_feat_dim, out_features=self.neighbor_co_occurrence_feat_dim))\n",
    "\n",
    "    def count_nodes_appearances(self, src_padded_nodes_neighbor_ids: np.ndarray, dst_padded_nodes_neighbor_ids: np.ndarray):\n",
    "        \"\"\"\n",
    "        count the appearances of nodes in the sequences of source and destination nodes\n",
    "        :param src_padded_nodes_neighbor_ids: ndarray, shape (batch_size, src_max_seq_length)\n",
    "        :param dst_padded_nodes_neighbor_ids:: ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # two lists to store the appearances of source and destination nodes\n",
    "        src_padded_nodes_appearances, dst_padded_nodes_appearances = [], []\n",
    "        # src_padded_node_neighbor_ids, ndarray, shape (src_max_seq_length, )\n",
    "        # dst_padded_node_neighbor_ids, ndarray, shape (dst_max_seq_length, )\n",
    "        for src_padded_node_neighbor_ids, dst_padded_node_neighbor_ids in zip(src_padded_nodes_neighbor_ids, dst_padded_nodes_neighbor_ids):\n",
    "\n",
    "            # src_unique_keys, ndarray, shape (num_src_unique_keys, )\n",
    "            # src_inverse_indices, ndarray, shape (src_max_seq_length, )\n",
    "            # src_counts, ndarray, shape (num_src_unique_keys, )\n",
    "            # we can use src_unique_keys[src_inverse_indices] to reconstruct the original input, and use src_counts[src_inverse_indices] to get counts of the original input\n",
    "            src_unique_keys, src_inverse_indices, src_counts = np.unique(src_padded_node_neighbor_ids, return_inverse=True, return_counts=True)\n",
    "            # Tensor, shape (src_max_seq_length, )\n",
    "            src_padded_node_neighbor_counts_in_src = torch.from_numpy(src_counts[src_inverse_indices]).float().to(self.device)\n",
    "            # dictionary, store the mapping relation from unique neighbor id to its appearances for the source node\n",
    "            src_mapping_dict = dict(zip(src_unique_keys, src_counts))\n",
    "\n",
    "            # dst_unique_keys, ndarray, shape (num_dst_unique_keys, )\n",
    "            # dst_inverse_indices, ndarray, shape (dst_max_seq_length, )\n",
    "            # dst_counts, ndarray, shape (num_dst_unique_keys, )\n",
    "            # we can use dst_unique_keys[dst_inverse_indices] to reconstruct the original input, and use dst_counts[dst_inverse_indices] to get counts of the original input\n",
    "            dst_unique_keys, dst_inverse_indices, dst_counts = np.unique(dst_padded_node_neighbor_ids, return_inverse=True, return_counts=True)\n",
    "            # Tensor, shape (dst_max_seq_length, )\n",
    "            dst_padded_node_neighbor_counts_in_dst = torch.from_numpy(dst_counts[dst_inverse_indices]).float().to(self.device)\n",
    "            # dictionary, store the mapping relation from unique neighbor id to its appearances for the destination node\n",
    "            dst_mapping_dict = dict(zip(dst_unique_keys, dst_counts))\n",
    "\n",
    "            # we need to use copy() to avoid the modification of src_padded_node_neighbor_ids\n",
    "            # Tensor, shape (src_max_seq_length, )\n",
    "            src_padded_node_neighbor_counts_in_dst = torch.from_numpy(src_padded_node_neighbor_ids.copy()).apply_(lambda neighbor_id: dst_mapping_dict.get(neighbor_id, 0.0)).float().to(self.device)\n",
    "            # Tensor, shape (src_max_seq_length, 2)\n",
    "            src_padded_nodes_appearances.append(torch.stack([src_padded_node_neighbor_counts_in_src, src_padded_node_neighbor_counts_in_dst], dim=1))\n",
    "\n",
    "            # we need to use copy() to avoid the modification of dst_padded_node_neighbor_ids\n",
    "            # Tensor, shape (dst_max_seq_length, )\n",
    "            dst_padded_node_neighbor_counts_in_src = torch.from_numpy(dst_padded_node_neighbor_ids.copy()).apply_(lambda neighbor_id: src_mapping_dict.get(neighbor_id, 0.0)).float().to(self.device)\n",
    "            # Tensor, shape (dst_max_seq_length, 2)\n",
    "            dst_padded_nodes_appearances.append(torch.stack([dst_padded_node_neighbor_counts_in_src, dst_padded_node_neighbor_counts_in_dst], dim=1))\n",
    "\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances = torch.stack(src_padded_nodes_appearances, dim=0)\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        dst_padded_nodes_appearances = torch.stack(dst_padded_nodes_appearances, dim=0)\n",
    "\n",
    "        # set the appearances of the padded node (with zero index) to zeros\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances[torch.from_numpy(src_padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        dst_padded_nodes_appearances[torch.from_numpy(dst_padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "\n",
    "        return src_padded_nodes_appearances, dst_padded_nodes_appearances\n",
    "\n",
    "    def forward(self, src_padded_nodes_neighbor_ids: np.ndarray, dst_padded_nodes_neighbor_ids: np.ndarray):\n",
    "        \"\"\"\n",
    "        compute the neighbor co-occurrence features of nodes in src_padded_nodes_neighbor_ids and dst_padded_nodes_neighbor_ids\n",
    "        :param src_padded_nodes_neighbor_ids: ndarray, shape (batch_size, src_max_seq_length)\n",
    "        :param dst_padded_nodes_neighbor_ids:: ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # src_padded_nodes_appearances, Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        # dst_padded_nodes_appearances, Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances, dst_padded_nodes_appearances = self.count_nodes_appearances(src_padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                                                                                                  dst_padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids)\n",
    "\n",
    "        # sum the neighbor co-occurrence features in the sequence of source and destination nodes\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        src_padded_nodes_neighbor_co_occurrence_features = self.neighbor_co_occurrence_encode_layer(src_padded_nodes_appearances.unsqueeze(dim=-1)).sum(dim=2)\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        dst_padded_nodes_neighbor_co_occurrence_features = self.neighbor_co_occurrence_encode_layer(dst_padded_nodes_appearances.unsqueeze(dim=-1)).sum(dim=2)\n",
    "\n",
    "        # src_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        return src_padded_nodes_neighbor_co_occurrence_features, dst_padded_nodes_neighbor_co_occurrence_features\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer encoder.\n",
    "        :param attention_dim: int, dimension of the attention vector\n",
    "        :param num_heads: int, number of attention heads\n",
    "        :param dropout: float, dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # use the MultiheadAttention implemented by PyTorch\n",
    "        self.multi_head_attention = MultiheadAttention(embed_dim=attention_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=attention_dim, out_features=4 * attention_dim),\n",
    "            nn.Linear(in_features=4 * attention_dim, out_features=attention_dim)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.LayerNorm(attention_dim),\n",
    "            nn.LayerNorm(attention_dim)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        encode the inputs by Transformer encoder\n",
    "        :param inputs: Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # note that the MultiheadAttention module accept input data with shape (seq_length, batch_size, input_dim), so we need to transpose the input\n",
    "        # Tensor, shape (num_patches, batch_size, self.attention_dim)\n",
    "        transposed_inputs = inputs.transpose(0, 1)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        transposed_inputs = self.norm_layers[0](transposed_inputs)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        hidden_states = self.multi_head_attention(query=transposed_inputs, key=transposed_inputs, value=transposed_inputs)[0].transpose(0, 1)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        outputs = inputs + self.dropout(hidden_states)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        hidden_states = self.linear_layers[1](self.dropout(F.gelu(self.linear_layers[0](self.norm_layers[1](outputs)))))\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        outputs = outputs + self.dropout(hidden_states)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shutil\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.TGAT import TGAT\n",
    "from models.MemoryModel import MemoryModel, compute_src_dst_node_time_shifts\n",
    "from models.CAWN import CAWN\n",
    "from models.TCL import TCL\n",
    "from models.GraphMixer import GraphMixer\n",
    "from models.DyGFormer import DyGFormer\n",
    "from models.modules import MergeLayer\n",
    "from utils.utils import set_random_seed, convert_to_gpu, get_parameter_sizes, create_optimizer\n",
    "from utils.utils import get_neighbor_sampler, NegativeEdgeSampler\n",
    "from evaluate_models_utils import evaluate_model_link_prediction\n",
    "from utils.metrics import get_link_prediction_metrics\n",
    "from utils.DataLoader import get_idx_data_loader, get_link_prediction_data\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.load_configs import get_link_prediction_args\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray, edge_ids: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        Data object to store the nodes interaction information.\n",
    "        :param src_node_ids: ndarray\n",
    "        :param dst_node_ids: ndarray\n",
    "        :param node_interact_times: ndarray\n",
    "        :param edge_ids: ndarray\n",
    "        :param labels: ndarray\n",
    "        \"\"\"\n",
    "        self.src_node_ids = src_node_ids\n",
    "        self.dst_node_ids = dst_node_ids\n",
    "        self.node_interact_times = node_interact_times\n",
    "        self.edge_ids = edge_ids\n",
    "        self.labels = labels\n",
    "        self.num_interactions = len(src_node_ids)\n",
    "        self.unique_node_ids = set(src_node_ids) | set(dst_node_ids)\n",
    "        self.num_unique_nodes = len(self.unique_node_ids)\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CustomizedDataset(Dataset):\n",
    "    def __init__(self, indices_list: list):\n",
    "        \"\"\"\n",
    "        Customized dataset.\n",
    "        :param indices_list: list, list of indices\n",
    "        \"\"\"\n",
    "        super(CustomizedDataset, self).__init__()\n",
    "\n",
    "        self.indices_list = indices_list\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        get item at the index in self.indices_list\n",
    "        :param idx: int, the index\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.indices_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_list)\n",
    "\n",
    "\n",
    "def get_idx_data_loader(indices_list: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    get data loader that iterates over indices\n",
    "    :param indices_list: list, list of indices\n",
    "    :param batch_size: int, batch size\n",
    "    :param shuffle: boolean, whether to shuffle the data\n",
    "    :return: data_loader, DataLoader\n",
    "    \"\"\"\n",
    "    dataset = CustomizedDataset(indices_list=indices_list)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_snpashot_idx_data_loaders(datasets: dict, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    get data loader that iterates over indices\n",
    "    :param indices_list: list, list of indices\n",
    "    :param batch_size: int, batch size\n",
    "    :param shuffle: boolean, whether to shuffle the data\n",
    "    :return: data_loader, DataLoader\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for ts in datasets.keys():  \n",
    "        dataset = CustomizedDataset(indices_list=list(range(len(datasets[ts]['edges'].src_node_ids))))\n",
    "\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=False)\n",
    "        out[ts] = data_loader\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(df):\n",
    "\n",
    "    src_node_ids = df.u.values.astype(np.long)\n",
    "\n",
    "    dst_node_ids = df.i.values.astype(np.long)\n",
    "    node_interact_times = df.ts.values.astype(np.float64)\n",
    "    edge_ids = df.idx.values.astype(np.long)\n",
    "    labels = df.label.values\n",
    "\n",
    "    nodes_set = set(src_node_ids) | set(dst_node_ids)\n",
    "    num_total_unique_node_ids = len(nodes_set)\n",
    "    src_node_ids = df.u.values.astype(np.long)\n",
    "\n",
    "    dst_node_ids = df.i.values.astype(np.long)\n",
    "    node_interact_times = df.ts.values.astype(np.float64)\n",
    "    edge_ids = np.array(list(range(len(df.idx.values)))).astype(np.long)\n",
    "    labels = df.label.values\n",
    "\n",
    "    train_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids,\n",
    "                      node_interact_times=node_interact_times,\n",
    "                    edge_ids=edge_ids, labels=labels)\n",
    "    return train_data\n",
    "    \n",
    "\n",
    "\n",
    "def get_dataset_subset(graph_df,percentage):\n",
    "    grouped = graph_df.groupby('ts')\n",
    "\n",
    "    # Define a function to sample each group proportionally\n",
    "\n",
    "\n",
    "    def percentage_sample(group, percentage):\n",
    "        n = int(len(group) * percentage)\n",
    "        return group.sample(n)\n",
    "\n",
    "\n",
    "    # Apply the proportional_sample function to each group\n",
    "    sampled = grouped.apply(percentage_sample, percentage=percentage).reset_index(drop=True)\n",
    "    graph_df =  graph_df[~graph_df.isin(sampled)].dropna()\n",
    "    return sampled,graph_df\n",
    "\n",
    "def make_data_dictionaries(graph_dfs , d, edge_raw_features,node_raw_features ,time_varying_features):\n",
    "    prev_edges = 0\n",
    "\n",
    "    NODE_FEAT_DIM = EDGE_FEAT_DIM = 172\n",
    "    for ts,graph_df in enumerate(graph_dfs.items()):\n",
    "        graph_df = graph_df[1]\n",
    "\n",
    "\n",
    "        new_df = pd.DataFrame(edge_raw_features[prev_edges:prev_edges + len(graph_df)])\n",
    "        new_df.reset_index(drop=True, inplace=True)\n",
    "   \n",
    "        graph_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "       \n",
    "        graph_df = pd.concat([graph_df, new_df], axis=1)\n",
    " \n",
    "\n",
    "        orginal_length = len(graph_df)\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "        if not time_varying_features:\n",
    "            if node_raw_features.shape[1 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features.shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_features[ts] = np.concatenate([node_raw_features, node_zero_padding], axis=1)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if node_raw_features.shape[2 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features[ts].shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_raw_features = np.concatenate([node_raw_features[ts], node_zero_padding], axis=1)\n",
    "\n",
    "        if edge_raw_features.shape[1] < EDGE_FEAT_DIM:\n",
    "                edge_zero_padding = np.zeros((edge_raw_features.shape[0], 172 - edge_raw_features.shape[1]))\n",
    "                edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)\n",
    "\n",
    "        assert NODE_FEAT_DIM == node_raw_features.shape[1] and EDGE_FEAT_DIM == edge_raw_features.shape[1], \"Unaligned feature dimensions after feature padding!\"\n",
    "   \n",
    "        d[ts]= {'edges': make_data(graph_df), 'node_features' : node_raw_features , 'edge_features':edge_raw_features}\n",
    "        prev_edges += len(graph_df)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def get_link_prediction_data_snapshots(dataset_name: str, val_ratio: float, test_ratio: float):\n",
    "    \"\"\"\n",
    "    generate data for link prediction task (inductive & transductive settings)\n",
    "    :param dataset_name: str, dataset name\n",
    "    :param val_ratio: float, validation data ratio\n",
    "    :param test_ratio: float, test data ratio\n",
    "    :return: node_raw_features, edge_raw_features, (np.ndarray),\n",
    "            full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data, (Data object)\n",
    "    \"\"\"\n",
    "    # Load data and train val test split\n",
    "    graph_df = pd.read_csv('./processed_data/{}/ml_{}.csv'.format(dataset_name, dataset_name))\n",
    "    \n",
    "    edge_raw_features = np.load('./processed_data/{}/ml_{}.npy'.format(dataset_name, dataset_name))\n",
    "    node_raw_features = np.load('./processed_data/{}/ml_{}_node.npy'.format(dataset_name, dataset_name))\n",
    "    cur_edges = 0\n",
    "    full_data_unstacked = make_data(graph_df)\n",
    "    if len(node_raw_features.shape) > 2: \n",
    "        time_varying_features = True\n",
    "    else: \n",
    "        time_varying_features = False\n",
    "  \n",
    "    val_graph_df , train_graph_df = get_dataset_subset(graph_df, val_ratio)\n",
    "\n",
    "    test_graph_df , train_graph_df = get_dataset_subset(train_graph_df, (len(graph_df) * test_ratio ) /len(train_graph_df))\n",
    "    train_data_unstacked = make_data(train_graph_df)\n",
    "    test_data_unstacked = make_data(test_graph_df)\n",
    "    val_data_unstacked = make_data(val_graph_df)\n",
    "\n",
    "    train_graph_dfs = dict(tuple(train_graph_df.groupby('ts')))\n",
    "    val_graph_dfs = dict(tuple(val_graph_df.groupby('ts')))\n",
    "    test_graph_dfs = dict(tuple(test_graph_df.groupby('ts')))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "    train_data = make_data_dictionaries(train_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    val_data = make_data_dictionaries(val_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    test_data  =make_data_dictionaries(test_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "\n",
    "    return full_data_unstacked,train_data_unstacked,train_data,val_data,test_data\n",
    "  \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# get arguments\n",
    "args = get_link_prediction_args(is_evaluation=False)\n",
    "\n",
    "# get data for training, validation and testing\n",
    "full_data_unstacked,train_data_unstacked,train_datas,val_datas,test_datas= \\\n",
    "    get_link_prediction_data_snapshots(dataset_name=args.dataset_name, val_ratio=args.val_ratio, test_ratio=args.test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_neighbor_sampler = get_neighbor_sampler(data=train_data_unstacked, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                time_scaling_factor=args.time_scaling_factor, seed=0)\n",
    "\n",
    "\n",
    "full_neighbor_sampler = get_neighbor_sampler(data=full_data_unstacked, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                time_scaling_factor=args.time_scaling_factor, seed=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=train_data_unstacked.src_node_ids, dst_node_ids=train_data_unstacked.dst_node_ids)\n",
    "val_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data_unstacked.src_node_ids, dst_node_ids=full_data_unstacked.dst_node_ids, seed=0)\n",
    "test_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data_unstacked.src_node_ids, dst_node_ids=full_data_unstacked.dst_node_ids, seed=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([475, 254,  87, ..., 175, 159, 354])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datas[0]['edges'].src_node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get data loaders\n",
    "\n",
    "train_data_loaders = get_snpashot_idx_data_loaders(train_datas, batch_size=args.batch_size, shuffle=False)\n",
    "val_data_loaders = get_snpashot_idx_data_loaders(val_datas, batch_size=args.batch_size, shuffle=False)\n",
    "test_data_loaders = get_snpashot_idx_data_loaders(test_datas,  batch_size=args.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:configuration is Namespace(batch_size=200, channel_embedding_dim=50, dataset_name='CanParl', device='cpu', dropout=0.1, edge_bank_memory_mode='unlimited_memory', gpu=0, learning_rate=0.0001, load_best_configs=False, max_input_sequence_length=32, model_name='DyGFormer', negative_sample_strategy='random', num_epochs=30, num_heads=2, num_layers=2, num_neighbors=20, num_runs=5, num_walk_heads=8, optimizer='Adam', patch_size=1, patience=20, position_feat_dim=172, sample_neighbor_strategy='recent', save_model_name='DyGFormer', test_interval_epochs=10, test_ratio=0.15, time_feat_dim=100, time_gap=2000, time_scaling_factor=1e-06, time_window_mode='fixed_proportion', val_ratio=0.15, walk_length=1, weight_decay=0.0)\n",
      "INFO:root:model -> Sequential(\n",
      "  (0): DyGFormer(\n",
      "    (time_encoder): TimeEncoder(\n",
      "      (w): Linear(in_features=1, out_features=100, bias=True)\n",
      "    )\n",
      "    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(\n",
      "      (neighbor_co_occurrence_encode_layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection_layer): ModuleDict(\n",
      "      (node): Linear(in_features=172, out_features=50, bias=True)\n",
      "      (edge): Linear(in_features=172, out_features=50, bias=True)\n",
      "      (time): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "    (transformers): ModuleList(\n",
      "      (0): TransformerEncoder(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=200, out_features=800, bias=True)\n",
      "          (1): Linear(in_features=800, out_features=200, bias=True)\n",
      "        )\n",
      "        (norm_layers): ModuleList(\n",
      "          (0): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoder(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=200, out_features=800, bias=True)\n",
      "          (1): Linear(in_features=800, out_features=200, bias=True)\n",
      "        )\n",
      "        (norm_layers): ModuleList(\n",
      "          (0): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=200, out_features=172, bias=True)\n",
      "  )\n",
      "  (1): MergeLayer(\n",
      "    (fc1): Linear(in_features=344, out_features=172, bias=True)\n",
      "    (fc2): Linear(in_features=172, out_features=1, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      ")\n",
      "INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_metric_all_runs, new_node_val_metric_all_runs, test_metric_all_runs, new_node_test_metric_all_runs = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.save_model_name = f'{args.model_name}'\n",
    "\n",
    "# set up logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "os.makedirs(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\", exist_ok=True)\n",
    "# create file handler that logs debug and higher level messages\n",
    "fh = logging.FileHandler(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/{str(time.time())}.log\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.WARNING)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "run_start_time = time.time()\n",
    "\n",
    "\n",
    "logger.info(f'configuration is {args}')\n",
    "\n",
    "\n",
    "node_feature_dim = train_datas[0]['node_features'].shape[1]\n",
    "edge_feature_dim = train_datas[0]['edge_features'].shape[1]\n",
    "\n",
    "dynamic_backbone = DyGFormer(node_raw_features=None, edge_raw_features=None, neighbor_sampler=train_neighbor_sampler,\n",
    "                                    time_feat_dim=args.time_feat_dim, channel_embedding_dim=args.channel_embedding_dim, patch_size=args.patch_size,\n",
    "                                    num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout,\n",
    "                                    max_input_sequence_length=args.max_input_sequence_length, device=args.device,\n",
    "                                    edge_feature_dim = train_datas[0]['edge_features'].shape[1],\n",
    "                                    node_feature_dim = train_datas[0]['node_features'].shape[1])\n",
    "\n",
    "link_predictor = MergeLayer(input_dim1=node_feature_dim, input_dim2=node_feature_dim,\n",
    "                            hidden_dim=node_feature_dim, output_dim=1)\n",
    "model = nn.Sequential(dynamic_backbone, link_predictor)\n",
    "logger.info(f'model -> {model}')\n",
    "logger.info(f'model name: {args.model_name}, #parameters: {get_parameter_sizes(model) * 4} B, '\n",
    "            f'{get_parameter_sizes(model) * 4 / 1024} KB, {get_parameter_sizes(model) * 4 / 1024 / 1024} MB.')\n",
    "\n",
    "optimizer = create_optimizer(model=model, optimizer_name=args.optimizer, learning_rate=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "model = convert_to_gpu(model, device=args.device)\n",
    "\n",
    "save_model_folder = f\"./saved_models/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\"\n",
    "shutil.rmtree(save_model_folder, ignore_errors=True)\n",
    "os.makedirs(save_model_folder, exist_ok=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=args.patience, save_model_folder=save_model_folder,\n",
    "                                save_model_name=args.save_model_name, logger=logger, model_name=args.model_name)\n",
    "\n",
    "loss_func = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, at timestamp : 0 train for the 9-th batch, train loss: 0.7082639336585999: 100%|█| 9/9 [00:01<00:00,  5.82it/s\n",
      "Epoch: 1, at timestamp : 1 train for the 26-th batch, train loss: 0.20162926614284515: 100%|█| 26/26 [00:44<00:00,  1.72\n",
      "Epoch: 1, at timestamp : 2 train for the 23-th batch, train loss: 0.37661921977996826: 100%|█| 23/23 [00:45<00:00,  1.96\n",
      "Epoch: 1, at timestamp : 3 train for the 16-th batch, train loss: 0.5609734654426575: 100%|█| 16/16 [00:31<00:00,  1.99s\n",
      "Epoch: 1, at timestamp : 4 train for the 34-th batch, train loss: 0.6407334804534912:  81%|▊| 34/42 [01:04<00:15,  1.90s"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "snapshots = train_data_loaders.keys()\n",
    "for epoch in range(args.num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    model[0].set_neighbor_sampler(train_neighbor_sampler)\n",
    "    for ts in snapshots :\n",
    "        train_idx_data_loader = train_data_loaders[ts]\n",
    "        # store train losses and metrics\n",
    "        train_data = train_datas[ts]['edges']\n",
    "        edge_features = train_datas[ts]['edge_features']\n",
    "        node_features = train_datas[ts]['node_features']\n",
    "        model[0].set_edge_node_features(node_features,edge_features)\n",
    "        train_losses, train_metrics = [], []\n",
    "        train_idx_data_loader_tqdm = tqdm(train_idx_data_loader, ncols=120)\n",
    "        \n",
    "        for batch_idx, train_data_indices in enumerate(train_idx_data_loader_tqdm):\n",
    "            batch_src_node_ids, batch_dst_node_ids, batch_node_interact_times, batch_edge_ids = \\\n",
    "                train_data.src_node_ids[train_data_indices], train_data.dst_node_ids[train_data_indices], \\\n",
    "                train_data.node_interact_times[train_data_indices], train_data.edge_ids[train_data_indices]\n",
    "\n",
    "            _, batch_neg_dst_node_ids = train_neg_edge_sampler.sample(size=len(batch_src_node_ids))\n",
    "            batch_neg_src_node_ids = batch_src_node_ids\n",
    "\n",
    "            \n",
    "            \n",
    "            # get temporal embedding of source and destination nodes\n",
    "            # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "            batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                    dst_node_ids=batch_dst_node_ids,\n",
    "                                                                    node_interact_times=batch_node_interact_times)\n",
    "\n",
    "            # get temporal embedding of negative source and negative destination nodes\n",
    "            # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "            batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                    dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                    node_interact_times=batch_node_interact_times)\n",
    "            \n",
    "            \n",
    "            positive_probabilities = model[1](input_1=batch_src_node_embeddings, input_2=batch_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "            negative_probabilities = model[1](input_1=batch_neg_src_node_embeddings, input_2=batch_neg_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "\n",
    "            predicts = torch.cat([positive_probabilities, negative_probabilities], dim=0)\n",
    "            labels = torch.cat([torch.ones_like(positive_probabilities), torch.zeros_like(negative_probabilities)], dim=0)\n",
    "            try:\n",
    "                loss = loss_func(input=predicts, target=labels)\n",
    "            except Exception:\n",
    "\n",
    "                print(predicts,labels)\n",
    "                raise Exception\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            train_metrics.append(get_link_prediction_metrics(predicts=predicts, labels=labels))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_idx_data_loader_tqdm.set_description(f'Epoch: {epoch + 1}, at timestamp : {ts} train for the {batch_idx + 1}-th batch, train loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2689448030.py, line 334)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[65], line 334\u001b[0;36m\u001b[0m\n\u001b[0;31m    for metric_name in val_metric_all_runs[0].keys():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#         val_losses, val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "#                                                                     model=model,\n",
    "#                                                                     neighbor_sampler=full_neighbor_sampler,\n",
    "#                                                                     evaluate_idx_data_loader=val_idx_data_loader,\n",
    "#                                                                     evaluate_neg_edge_sampler=val_neg_edge_sampler,\n",
    "#                                                                     evaluate_data=val_data,\n",
    "#                                                                     loss_func=loss_func,\n",
    "#                                                                     num_neighbors=args.num_neighbors,\n",
    "#                                                                     time_gap=args.time_gap)\n",
    "# )\n",
    "\n",
    "#         new_node_val_losses, new_node_val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "#                                                                                     model=model,\n",
    "#                                                                                     neighbor_sampler=full_neighbor_sampler,\n",
    "#                                                                                     evaluate_idx_data_loader=new_node_val_idx_data_loader,\n",
    "#                                                                                     evaluate_neg_edge_sampler=new_node_val_neg_edge_sampler,\n",
    "#                                                                                     evaluate_data=new_node_val_data,\n",
    "#                                                                                     loss_func=loss_func,\n",
    "#                                                                                     num_neighbors=args.num_neighbors,\n",
    "#                                                                                     time_gap=args.time_gap)\n",
    "\n",
    "      \n",
    "#         logger.info(f'Epoch: {epoch + 1}, learning rate: {optimizer.param_groups[0][\"lr\"]}, train loss: {np.mean(train_losses):.4f}')\n",
    "#         for metric_name in train_metrics[0].keys():\n",
    "#             logger.info(f'train {metric_name}, {np.mean([train_metric[metric_name] for train_metric in train_metrics]):.4f}')\n",
    "#         logger.info(f'validate loss: {np.mean(val_losses):.4f}')\n",
    "#         for metric_name in val_metrics[0].keys():\n",
    "#             logger.info(f'validate {metric_name}, {np.mean([val_metric[metric_name] for val_metric in val_metrics]):.4f}')\n",
    "#         logger.info(f'new node validate loss: {np.mean(new_node_val_losses):.4f}')\n",
    "#         for metric_name in new_node_val_metrics[0].keys():\n",
    "#             logger.info(f'new node validate {metric_name}, {np.mean([new_node_val_metric[metric_name] for new_node_val_metric in new_node_val_metrics]):.4f}')\n",
    "\n",
    "#         # perform testing once after test_interval_epochs\n",
    "#         if (epoch + 1) % args.test_interval_epochs == 0:\n",
    "#             test_losses, test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "#                                                                         model=model,\n",
    "#                                                                         neighbor_sampler=full_neighbor_sampler,\n",
    "#                                                                         evaluate_idx_data_loader=test_idx_data_loader,\n",
    "#                                                                         evaluate_neg_edge_sampler=test_neg_edge_sampler,\n",
    "#                                                                         evaluate_data=test_data,\n",
    "#                                                                         loss_func=loss_func,\n",
    "#                                                                         num_neighbors=args.num_neighbors,\n",
    "#                                                                         time_gap=args.time_gap)\n",
    "\n",
    "            \n",
    "\n",
    "#             new_node_test_losses, new_node_test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "#                                                                                             model=model,\n",
    "#                                                                                             neighbor_sampler=full_neighbor_sampler,\n",
    "#                                                                                             evaluate_idx_data_loader=new_node_test_idx_data_loader,\n",
    "#                                                                                             evaluate_neg_edge_sampler=new_node_test_neg_edge_sampler,\n",
    "#                                                                                             evaluate_data=new_node_test_data,\n",
    "#                                                                                             loss_func=loss_func,\n",
    "#                                                                                             num_neighbors=args.num_neighbors,\n",
    "#                                                                                             time_gap=args.time_gap)\n",
    "\n",
    "        #     if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "        #         # reload validation memory bank for testing nodes or saving models\n",
    "        #         # note that since model treats memory as parameters, we need to reload the memory to val_backup_memory_bank for saving models\n",
    "        #         model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "        #     logger.info(f'test loss: {np.mean(test_losses):.4f}')\n",
    "        #     for metric_name in test_metrics[0].keys():\n",
    "        #         logger.info(f'test {metric_name}, {np.mean([test_metric[metric_name] for test_metric in test_metrics]):.4f}')\n",
    "        #     logger.info(f'new node test loss: {np.mean(new_node_test_losses):.4f}')\n",
    "        #     for metric_name in new_node_test_metrics[0].keys():\n",
    "        #         logger.info(f'new node test {metric_name}, {np.mean([new_node_test_metric[metric_name] for new_node_test_metric in new_node_test_metrics]):.4f}')\n",
    "\n",
    "        # # select the best model based on all the validate metrics\n",
    "        # val_metric_indicator = []\n",
    "        # for metric_name in val_metrics[0].keys():\n",
    "        #     val_metric_indicator.append((metric_name, np.mean([val_metric[metric_name] for val_metric in val_metrics]), True))\n",
    "        # early_stop = early_stopping.step(val_metric_indicator, model)\n",
    "\n",
    "        # if early_stop:\n",
    "            break\n",
    "\n",
    "# load the best model\n",
    "early_stopping.load_checkpoint(model)\n",
    "\n",
    "# evaluate the best model\n",
    "logger.info(f'get final performance on dataset {args.dataset_name}...')\n",
    "\n",
    "# the saved best model of memory-based models cannot perform validation since the stored memory has been updated by validation data\n",
    "if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "    val_losses, val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                model=model,\n",
    "                                                                neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                evaluate_idx_data_loader=val_idx_data_loader,\n",
    "                                                                evaluate_neg_edge_sampler=val_neg_edge_sampler,\n",
    "                                                                evaluate_data=val_data,\n",
    "                                                                loss_func=loss_func,\n",
    "                                                                num_neighbors=args.num_neighbors,\n",
    "                                                                time_gap=args.time_gap)\n",
    "\n",
    "    new_node_val_losses, new_node_val_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                model=model,\n",
    "                                                                                neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                evaluate_idx_data_loader=new_node_val_idx_data_loader,\n",
    "                                                                                evaluate_neg_edge_sampler=new_node_val_neg_edge_sampler,\n",
    "                                                                                evaluate_data=new_node_val_data,\n",
    "                                                                                loss_func=loss_func,\n",
    "                                                                                num_neighbors=args.num_neighbors,\n",
    "                                                                                time_gap=args.time_gap)\n",
    "\n",
    "if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "    # the memory in the best model has seen the validation edges, we need to backup the memory for new testing nodes\n",
    "    val_backup_memory_bank = model[0].memory_bank.backup_memory_bank()\n",
    "\n",
    "test_losses, test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                            model=model,\n",
    "                                                            neighbor_sampler=full_neighbor_sampler,\n",
    "                                                            evaluate_idx_data_loader=test_idx_data_loader,\n",
    "                                                            evaluate_neg_edge_sampler=test_neg_edge_sampler,\n",
    "                                                            evaluate_data=test_data,\n",
    "                                                            loss_func=loss_func,\n",
    "                                                            num_neighbors=args.num_neighbors,\n",
    "                                                            time_gap=args.time_gap)\n",
    "\n",
    "if args.model_name in ['JODIE', 'DyRep', 'TGN']:\n",
    "    # reload validation memory bank for new testing nodes\n",
    "    model[0].memory_bank.reload_memory_bank(val_backup_memory_bank)\n",
    "\n",
    "new_node_test_losses, new_node_test_metrics = evaluate_model_link_prediction(model_name=args.model_name,\n",
    "                                                                                model=model,\n",
    "                                                                                neighbor_sampler=full_neighbor_sampler,\n",
    "                                                                                evaluate_idx_data_loader=new_node_test_idx_data_loader,\n",
    "                                                                                evaluate_neg_edge_sampler=new_node_test_neg_edge_sampler,\n",
    "                                                                                evaluate_data=new_node_test_data,\n",
    "                                                                                loss_func=loss_func,\n",
    "                                                                                num_neighbors=args.num_neighbors,\n",
    "                                                                                time_gap=args.time_gap)\n",
    "# store the evaluation metrics at the current run\n",
    "val_metric_dict, new_node_val_metric_dict, test_metric_dict, new_node_test_metric_dict = {}, {}, {}, {}\n",
    "\n",
    "if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "    logger.info(f'validate loss: {np.mean(val_losses):.4f}')\n",
    "    for metric_name in val_metrics[0].keys():\n",
    "        average_val_metric = np.mean([val_metric[metric_name] for val_metric in val_metrics])\n",
    "        logger.info(f'validate {metric_name}, {average_val_metric:.4f}')\n",
    "        val_metric_dict[metric_name] = average_val_metric\n",
    "\n",
    "    logger.info(f'new node validate loss: {np.mean(new_node_val_losses):.4f}')\n",
    "    for metric_name in new_node_val_metrics[0].keys():\n",
    "        average_new_node_val_metric = np.mean([new_node_val_metric[metric_name] for new_node_val_metric in new_node_val_metrics])\n",
    "        logger.info(f'new node validate {metric_name}, {average_new_node_val_metric:.4f}')\n",
    "        new_node_val_metric_dict[metric_name] = average_new_node_val_metric\n",
    "\n",
    "logger.info(f'test loss: {np.mean(test_losses):.4f}')\n",
    "for metric_name in test_metrics[0].keys():\n",
    "    average_test_metric = np.mean([test_metric[metric_name] for test_metric in test_metrics])\n",
    "    logger.info(f'test {metric_name}, {average_test_metric:.4f}')\n",
    "    test_metric_dict[metric_name] = average_test_metric\n",
    "\n",
    "logger.info(f'new node test loss: {np.mean(new_node_test_losses):.4f}')\n",
    "for metric_name in new_node_test_metrics[0].keys():\n",
    "    average_new_node_test_metric = np.mean([new_node_test_metric[metric_name] for new_node_test_metric in new_node_test_metrics])\n",
    "    logger.info(f'new node test {metric_name}, {average_new_node_test_metric:.4f}')\n",
    "    new_node_test_metric_dict[metric_name] = average_new_node_test_metric\n",
    "\n",
    "single_run_time = time.time() - run_start_time\n",
    "logger.info(f'Run {run + 1} cost {single_run_time:.2f} seconds.')\n",
    "\n",
    "if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "    val_metric_all_runs.append(val_metric_dict)\n",
    "    new_node_val_metric_all_runs.append(new_node_val_metric_dict)\n",
    "test_metric_all_runs.append(test_metric_dict)\n",
    "new_node_test_metric_all_runs.append(new_node_test_metric_dict)\n",
    "\n",
    "# avoid the overlap of logs\n",
    "if run < args.num_runs - 1:\n",
    "    logger.removeHandler(fh)\n",
    "    logger.removeHandler(ch)\n",
    "\n",
    "# save model result\n",
    "if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "    result_json = {\n",
    "        \"validate metrics\": {metric_name: f'{val_metric_dict[metric_name]:.4f}' for metric_name in val_metric_dict},\n",
    "        \"new node validate metrics\": {metric_name: f'{new_node_val_metric_dict[metric_name]:.4f}' for metric_name in new_node_val_metric_dict},\n",
    "        \"test metrics\": {metric_name: f'{test_metric_dict[metric_name]:.4f}' for metric_name in test_metric_dict},\n",
    "        \"new node test metrics\": {metric_name: f'{new_node_test_metric_dict[metric_name]:.4f}' for metric_name in new_node_test_metric_dict}\n",
    "    }\n",
    "else:\n",
    "    result_json = {\n",
    "        \"test metrics\": {metric_name: f'{test_metric_dict[metric_name]:.4f}' for metric_name in test_metric_dict},\n",
    "        \"new node test metrics\": {metric_name: f'{new_node_test_metric_dict[metric_name]:.4f}' for metric_name in new_node_test_metric_dict}\n",
    "    }\n",
    "result_json = json.dumps(result_json, indent=4)\n",
    "\n",
    "save_result_folder = f\"./saved_results/{args.model_name}/{args.dataset_name}\"\n",
    "os.makedirs(save_result_folder, exist_ok=True)\n",
    "save_result_path = os.path.join(save_result_folder, f\"{args.save_model_name}.json\")\n",
    "\n",
    "with open(save_result_path, 'w') as file:\n",
    "    file.write(result_json)\n",
    "\n",
    "# store the average metrics at the log of the last run\n",
    "logger.info(f'metrics over {args.num_runs} runs:')\n",
    "\n",
    "if args.model_name not in ['JODIE', 'DyRep', 'TGN']:\n",
    "for metric_name in val_metric_all_runs[0].keys():\n",
    "    logger.info(f'validate {metric_name}, {[val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs]}')\n",
    "    logger.info(f'average validate {metric_name}, {np.mean([val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs]):.4f} '\n",
    "                f'± {np.std([val_metric_single_run[metric_name] for val_metric_single_run in val_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "for metric_name in new_node_val_metric_all_runs[0].keys():\n",
    "    logger.info(f'new node validate {metric_name}, {[new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs]}')\n",
    "    logger.info(f'average new node validate {metric_name}, {np.mean([new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs]):.4f} '\n",
    "                f'± {np.std([new_node_val_metric_single_run[metric_name] for new_node_val_metric_single_run in new_node_val_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "for metric_name in test_metric_all_runs[0].keys():\n",
    "logger.info(f'test {metric_name}, {[test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs]}')\n",
    "logger.info(f'average test {metric_name}, {np.mean([test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs]):.4f} '\n",
    "            f'± {np.std([test_metric_single_run[metric_name] for test_metric_single_run in test_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "for metric_name in new_node_test_metric_all_runs[0].keys():\n",
    "logger.info(f'new node test {metric_name}, {[new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs]}')\n",
    "logger.info(f'average new node test {metric_name}, {np.mean([new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs]):.4f} '\n",
    "            f'± {np.std([new_node_test_metric_single_run[metric_name] for new_node_test_metric_single_run in new_node_test_metric_all_runs], ddof=1):.4f}')\n",
    "\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  value\n",
       "0        A      1\n",
       "1        B      4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with a categorical column 'category'\n",
    "df = pd.DataFrame({\n",
    "    'category': ['A', 'A', 'A', 'B', 'B', 'C'],\n",
    "    'value': [1, 2, 3, 4, 5, 6]\n",
    "})\n",
    "\n",
    "# Group the DataFrame by the 'category' column\n",
    "grouped = df.groupby('category')\n",
    "\n",
    "# Define a function to sample each group with a specified percentage\n",
    "def percentage_sample(group, percentage):\n",
    "    n = int(len(group) * percentage)\n",
    "    return group.sample(n)\n",
    "\n",
    "# Apply the percentage_sample function to each group\n",
    "percentage = 0.5\n",
    "sampled = grouped.apply(percentage_sample, percentage=percentage)\n",
    "\n",
    "# The original DataFrame is still in the variable df\n",
    "# The new DataFrame based on the sample is in the variable sampled\n",
    "sampled = sampled.reset_index(drop=True)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:configuration is Namespace(batch_size=200, channel_embedding_dim=50, dataset_name='CanParl', device='cpu', dropout=0.1, edge_bank_memory_mode='unlimited_memory', gpu=0, learning_rate=0.0001, load_best_configs=False, max_input_sequence_length=32, model_name='DyGFormer', negative_sample_strategy='random', num_epochs=30, num_heads=2, num_layers=2, num_neighbors=20, num_runs=5, num_walk_heads=8, optimizer='Adam', patch_size=1, patience=20, position_feat_dim=172, sample_neighbor_strategy='recent', save_model_name='DyGFormer', test_interval_epochs=10, test_ratio=0.15, time_feat_dim=100, time_gap=2000, time_scaling_factor=1e-06, time_window_mode='fixed_proportion', val_ratio=0.15, walk_length=1, weight_decay=0.0)\n",
      "INFO:root:model -> Sequential(\n",
      "  (0): DyGFormer(\n",
      "    (time_encoder): TimeEncoder(\n",
      "      (w): Linear(in_features=1, out_features=100, bias=True)\n",
      "    )\n",
      "    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(\n",
      "      (neighbor_co_occurrence_encode_layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection_layer): ModuleDict(\n",
      "      (node): Linear(in_features=172, out_features=50, bias=True)\n",
      "      (edge): Linear(in_features=172, out_features=50, bias=True)\n",
      "      (time): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "    (transformers): ModuleList(\n",
      "      (0): TransformerEncoder(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=200, out_features=800, bias=True)\n",
      "          (1): Linear(in_features=800, out_features=200, bias=True)\n",
      "        )\n",
      "        (norm_layers): ModuleList(\n",
      "          (0): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoder(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=200, out_features=800, bias=True)\n",
      "          (1): Linear(in_features=800, out_features=200, bias=True)\n",
      "        )\n",
      "        (norm_layers): ModuleList(\n",
      "          (0): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=200, out_features=172, bias=True)\n",
      "  )\n",
      "  (1): MergeLayer(\n",
      "    (fc1): Linear(in_features=344, out_features=172, bias=True)\n",
      "    (fc2): Linear(in_features=172, out_features=1, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      ")\n",
      "INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.\n",
      "Epoch: 1, at timestamp : 0 train for the 9-th batch, train loss: 0.6936125755310059: 100%|█| 9/9 [00:01<00:00,  5.92it/s\n",
      "Epoch: 1, at timestamp : 1 train for the 4-th batch, train loss: 0.6920541524887085:  15%|▏| 4/26 [00:07<00:40,  1.85s/i\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 873\u001b[0m\n\u001b[1;32m    865\u001b[0m batch_src_node_embeddings, batch_dst_node_embeddings \u001b[39m=\u001b[39m \\\n\u001b[1;32m    866\u001b[0m     model[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcompute_src_dst_node_temporal_embeddings(src_node_ids\u001b[39m=\u001b[39mbatch_src_node_ids,\n\u001b[1;32m    867\u001b[0m                                                         dst_node_ids\u001b[39m=\u001b[39mbatch_dst_node_ids,\n\u001b[1;32m    868\u001b[0m                                                         node_interact_times\u001b[39m=\u001b[39mbatch_node_interact_times)\n\u001b[1;32m    870\u001b[0m \u001b[39m# get temporal embedding of negative source and negative destination nodes\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[39m# two Tensors, with shape (batch_size, node_feat_dim)\u001b[39;00m\n\u001b[1;32m    872\u001b[0m batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 873\u001b[0m     model[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcompute_src_dst_node_temporal_embeddings(src_node_ids\u001b[39m=\u001b[39;49mbatch_neg_src_node_ids,\n\u001b[1;32m    874\u001b[0m                                                         dst_node_ids\u001b[39m=\u001b[39;49mbatch_neg_dst_node_ids,\n\u001b[1;32m    875\u001b[0m                                                         node_interact_times\u001b[39m=\u001b[39;49mbatch_node_interact_times)\n\u001b[1;32m    878\u001b[0m positive_probabilities \u001b[39m=\u001b[39m model[\u001b[39m1\u001b[39m](input_1\u001b[39m=\u001b[39mbatch_src_node_embeddings, input_2\u001b[39m=\u001b[39mbatch_dst_node_embeddings)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msigmoid()\n\u001b[1;32m    879\u001b[0m negative_probabilities \u001b[39m=\u001b[39m model[\u001b[39m1\u001b[39m](input_1\u001b[39m=\u001b[39mbatch_neg_src_node_embeddings, input_2\u001b[39m=\u001b[39mbatch_neg_dst_node_embeddings)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msigmoid()\n",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m, in \u001b[0;36mDyGFormer.compute_src_dst_node_temporal_embeddings\u001b[0;34m(self, src_node_ids, dst_node_ids, node_interact_times)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m# Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers:\n\u001b[0;32m--> 191\u001b[0m     patches_data \u001b[39m=\u001b[39m transformer(patches_data)\n\u001b[1;32m    193\u001b[0m \u001b[39m# src_patches_data, Tensor, shape (batch_size, src_num_patches, num_channels * channel_embedding_dim)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m src_patches_data \u001b[39m=\u001b[39m patches_data[:, : src_num_patches, :]\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 467\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    465\u001b[0m transposed_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_layers[\u001b[39m0\u001b[39m](transposed_inputs)\n\u001b[1;32m    466\u001b[0m \u001b[39m# Tensor, shape (batch_size, num_patches, self.attention_dim)\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_head_attention(query\u001b[39m=\u001b[39;49mtransposed_inputs, key\u001b[39m=\u001b[39;49mtransposed_inputs, value\u001b[39m=\u001b[39;49mtransposed_inputs)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[39m# Tensor, shape (batch_size, num_patches, self.attention_dim)\u001b[39;00m\n\u001b[1;32m    469\u001b[0m outputs \u001b[39m=\u001b[39m inputs \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1154\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1155\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1156\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1157\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1158\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1159\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1160\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/torch/nn/functional.py:5179\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5174\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m   5176\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   5177\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m-> 5179\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[1;32m   5180\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   5181\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m~/TGN/lib/python3.8/site-packages/torch/nn/functional.py:4860\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4858\u001b[0m     attn \u001b[39m=\u001b[39m dropout(attn, p\u001b[39m=\u001b[39mdropout_p)\n\u001b[1;32m   4859\u001b[0m \u001b[39m# (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\u001b[39;00m\n\u001b[0;32m-> 4860\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(attn, v)\n\u001b[1;32m   4861\u001b[0m \u001b[39mreturn\u001b[39;00m output, attn\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "from models.modules import TimeEncoder\n",
    "from utils.utils import NeighborSampler\n",
    "\n",
    "\n",
    "class DyGFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, node_raw_features: np.ndarray, edge_raw_features: np.ndarray, neighbor_sampler: NeighborSampler,\n",
    "                 time_feat_dim: int, channel_embedding_dim: int, patch_size: int = 1, num_layers: int = 2, num_heads: int = 2,\n",
    "                 dropout: float = 0.1, max_input_sequence_length: int = 512, device: str = 'cpu',node_feature_dim= None,edge_feature_dim= None ):\n",
    "        \"\"\"\n",
    "        DyGFormer model.\n",
    "        :param node_raw_features: ndarray, shape (num_nodes + 1, node_feat_dim)\n",
    "        :param edge_raw_features: ndarray, shape (num_edges + 1, edge_feat_dim)\n",
    "        :param neighbor_sampler: neighbor sampler\n",
    "        :param time_feat_dim: int, dimension of time features (encodings)\n",
    "        :param channel_embedding_dim: int, dimension of each channel embedding\n",
    "        :param patch_size: int, patch size\n",
    "        :param num_layers: int, number of transformer layers\n",
    "        :param num_heads: int, number of attention heads\n",
    "        :param dropout: float, dropout rate\n",
    "        :param max_input_sequence_length: int, maximal length of the input sequence for each node\n",
    "        :param device: str, device\n",
    "        \"\"\"\n",
    "        super(DyGFormer, self).__init__()\n",
    "        self.device = device\n",
    "        if edge_raw_features and node_raw_features:\n",
    "            self.node_raw_features = torch.from_numpy(node_raw_features.astype(np.float32)).to(device)\n",
    "            self.edge_raw_features = torch.from_numpy(edge_raw_features.astype(np.float32)).to(device)\n",
    "            self.node_feat_dim = self.node_raw_features.shape[1]\n",
    "            self.edge_feat_dim = self.edge_raw_features.shape[1]\n",
    "        if node_feature_dim and edge_feature_dim:\n",
    "            self.node_feat_dim = node_feature_dim \n",
    "            self.edge_feat_dim = edge_feature_dim\n",
    "\n",
    "\n",
    "        self.neighbor_sampler = neighbor_sampler\n",
    "        self.time_feat_dim = time_feat_dim\n",
    "        self.channel_embedding_dim = channel_embedding_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.max_input_sequence_length = max_input_sequence_length\n",
    "        self.device = device\n",
    "\n",
    "        self.time_encoder = TimeEncoder(time_dim=time_feat_dim)\n",
    "\n",
    "        self.neighbor_co_occurrence_feat_dim = self.channel_embedding_dim\n",
    "        self.neighbor_co_occurrence_encoder = NeighborCooccurrenceEncoder(neighbor_co_occurrence_feat_dim=self.neighbor_co_occurrence_feat_dim, device=self.device)\n",
    "\n",
    "        self.projection_layer = nn.ModuleDict({\n",
    "            'node': nn.Linear(in_features=self.patch_size * self.node_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'edge': nn.Linear(in_features=self.patch_size * self.edge_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'time': nn.Linear(in_features=self.patch_size * self.time_feat_dim, out_features=self.channel_embedding_dim, bias=True),\n",
    "            'neighbor_co_occurrence': nn.Linear(in_features=self.patch_size * self.neighbor_co_occurrence_feat_dim, out_features=self.channel_embedding_dim, bias=True)\n",
    "        })\n",
    "\n",
    "        self.num_channels = 4\n",
    "\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerEncoder(attention_dim=self.num_channels * self.channel_embedding_dim, num_heads=self.num_heads, dropout=self.dropout)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.num_channels * self.channel_embedding_dim, out_features=self.node_feat_dim, bias=True)\n",
    "    def set_edge_node_features(self,node_raw_features,edge_raw_features):\n",
    "        self.node_raw_features = torch.from_numpy(node_raw_features.astype(np.float32)).to(self.device)\n",
    "        self.edge_raw_features = torch.from_numpy(edge_raw_features.astype(np.float32)).to(self.device)\n",
    "      \n",
    "    \n",
    "\n",
    "         \n",
    "\n",
    "    def compute_src_dst_node_temporal_embeddings(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray):\n",
    "        \"\"\"\n",
    "        compute source and destination node temporal embeddings\n",
    "        :param src_node_ids: ndarray, shape (batch_size, )\n",
    "        :param dst_node_ids: ndarray, shape (batch_size, )\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get the first-hop neighbors of source and destination nodes\n",
    "        # three lists to store source nodes' first-hop neighbor ids, edge ids and interaction timestamp information, with batch_size as the list length\n",
    "        # print(src_node_ids, node_interact_times)\n",
    "        src_nodes_neighbor_ids_list, src_nodes_edge_ids_list, src_nodes_neighbor_times_list = \\\n",
    "            self.neighbor_sampler.get_all_first_hop_neighbors(node_ids=src_node_ids, node_interact_times=node_interact_times)\n",
    "\n",
    "        # three lists to store destination nodes' first-hop neighbor ids, edge ids and interaction timestamp information, with batch_size as the list length\n",
    "        dst_nodes_neighbor_ids_list, dst_nodes_edge_ids_list, dst_nodes_neighbor_times_list = \\\n",
    "            self.neighbor_sampler.get_all_first_hop_neighbors(node_ids=dst_node_ids, node_interact_times=node_interact_times)\n",
    "\n",
    "        # pad the sequences of first-hop neighbors for source and destination nodes\n",
    "        # src_padded_nodes_neighbor_ids, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        # src_padded_nodes_edge_ids, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        # src_padded_nodes_neighbor_times, ndarray, shape (batch_size, src_max_seq_length)\n",
    "        src_padded_nodes_neighbor_ids, src_padded_nodes_edge_ids, src_padded_nodes_neighbor_times = \\\n",
    "            self.pad_sequences(node_ids=src_node_ids, node_interact_times=node_interact_times, nodes_neighbor_ids_list=src_nodes_neighbor_ids_list,\n",
    "                               nodes_edge_ids_list=src_nodes_edge_ids_list, nodes_neighbor_times_list=src_nodes_neighbor_times_list,\n",
    "                               patch_size=self.patch_size, max_input_sequence_length=self.max_input_sequence_length)\n",
    "\n",
    "        # dst_padded_nodes_neighbor_ids, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        # dst_padded_nodes_edge_ids, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        # dst_padded_nodes_neighbor_times, ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        dst_padded_nodes_neighbor_ids, dst_padded_nodes_edge_ids, dst_padded_nodes_neighbor_times = \\\n",
    "            self.pad_sequences(node_ids=dst_node_ids, node_interact_times=node_interact_times, nodes_neighbor_ids_list=dst_nodes_neighbor_ids_list,\n",
    "                               nodes_edge_ids_list=dst_nodes_edge_ids_list, nodes_neighbor_times_list=dst_nodes_neighbor_times_list,\n",
    "                               patch_size=self.patch_size, max_input_sequence_length=self.max_input_sequence_length)\n",
    "\n",
    "        # src_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        src_padded_nodes_neighbor_co_occurrence_features, dst_padded_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.neighbor_co_occurrence_encoder(src_padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                                                dst_padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids)\n",
    "\n",
    "        # get the features of the sequence of source and destination nodes\n",
    "        # src_padded_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, src_max_seq_length, node_feat_dim)\n",
    "        # src_padded_nodes_edge_raw_features, Tensor, shape (batch_size, src_max_seq_length, edge_feat_dim)\n",
    "        # src_padded_nodes_neighbor_time_features, Tensor, shape (batch_size, src_max_seq_length, time_feat_dim)\n",
    "        src_padded_nodes_neighbor_node_raw_features, src_padded_nodes_edge_raw_features, src_padded_nodes_neighbor_time_features = \\\n",
    "            self.get_features(node_interact_times=node_interact_times, padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                              padded_nodes_edge_ids=src_padded_nodes_edge_ids, padded_nodes_neighbor_times=src_padded_nodes_neighbor_times, time_encoder=self.time_encoder)\n",
    "\n",
    "        # dst_padded_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, dst_max_seq_length, node_feat_dim)\n",
    "        # dst_padded_nodes_edge_raw_features, Tensor, shape (batch_size, dst_max_seq_length, edge_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_time_features, Tensor, shape (batch_size, dst_max_seq_length, time_feat_dim)\n",
    "        dst_padded_nodes_neighbor_node_raw_features, dst_padded_nodes_edge_raw_features, dst_padded_nodes_neighbor_time_features = \\\n",
    "            self.get_features(node_interact_times=node_interact_times, padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids,\n",
    "                              padded_nodes_edge_ids=dst_padded_nodes_edge_ids, padded_nodes_neighbor_times=dst_padded_nodes_neighbor_times, time_encoder=self.time_encoder)\n",
    "\n",
    "        # get the patches for source and destination nodes\n",
    "        # src_patches_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, src_num_patches, patch_size * node_feat_dim)\n",
    "        # src_patches_nodes_edge_raw_features, Tensor, shape (batch_size, src_num_patches, patch_size * edge_feat_dim)\n",
    "        # src_patches_nodes_neighbor_time_features, Tensor, shape (batch_size, src_num_patches, patch_size * time_feat_dim)\n",
    "        src_patches_nodes_neighbor_node_raw_features, src_patches_nodes_edge_raw_features, \\\n",
    "        src_patches_nodes_neighbor_time_features, src_patches_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.get_patches(padded_nodes_neighbor_node_raw_features=src_padded_nodes_neighbor_node_raw_features,\n",
    "                             padded_nodes_edge_raw_features=src_padded_nodes_edge_raw_features,\n",
    "                             padded_nodes_neighbor_time_features=src_padded_nodes_neighbor_time_features,\n",
    "                             padded_nodes_neighbor_co_occurrence_features=src_padded_nodes_neighbor_co_occurrence_features,\n",
    "                             patch_size=self.patch_size)\n",
    "\n",
    "        # dst_patches_nodes_neighbor_node_raw_features, Tensor, shape (batch_size, dst_num_patches, patch_size * node_feat_dim)\n",
    "        # dst_patches_nodes_edge_raw_features, Tensor, shape (batch_size, dst_num_patches, patch_size * edge_feat_dim)\n",
    "        # dst_patches_nodes_neighbor_time_features, Tensor, shape (batch_size, dst_num_patches, patch_size * time_feat_dim)\n",
    "        dst_patches_nodes_neighbor_node_raw_features, dst_patches_nodes_edge_raw_features, \\\n",
    "        dst_patches_nodes_neighbor_time_features, dst_patches_nodes_neighbor_co_occurrence_features = \\\n",
    "            self.get_patches(padded_nodes_neighbor_node_raw_features=dst_padded_nodes_neighbor_node_raw_features,\n",
    "                             padded_nodes_edge_raw_features=dst_padded_nodes_edge_raw_features,\n",
    "                             padded_nodes_neighbor_time_features=dst_padded_nodes_neighbor_time_features,\n",
    "                             padded_nodes_neighbor_co_occurrence_features=dst_padded_nodes_neighbor_co_occurrence_features,\n",
    "                             patch_size=self.patch_size)\n",
    "\n",
    "        # align the patch encoding dimension\n",
    "        # Tensor, shape (batch_size, src_num_patches, channel_embedding_dim)\n",
    "        src_patches_nodes_neighbor_node_raw_features = self.projection_layer['node'](src_patches_nodes_neighbor_node_raw_features)\n",
    "        src_patches_nodes_edge_raw_features = self.projection_layer['edge'](src_patches_nodes_edge_raw_features)\n",
    "        src_patches_nodes_neighbor_time_features = self.projection_layer['time'](src_patches_nodes_neighbor_time_features)\n",
    "        src_patches_nodes_neighbor_co_occurrence_features = self.projection_layer['neighbor_co_occurrence'](src_patches_nodes_neighbor_co_occurrence_features)\n",
    "\n",
    "        # Tensor, shape (batch_size, dst_num_patches, channel_embedding_dim)\n",
    "        dst_patches_nodes_neighbor_node_raw_features = self.projection_layer['node'](dst_patches_nodes_neighbor_node_raw_features)\n",
    "        dst_patches_nodes_edge_raw_features = self.projection_layer['edge'](dst_patches_nodes_edge_raw_features)\n",
    "        dst_patches_nodes_neighbor_time_features = self.projection_layer['time'](dst_patches_nodes_neighbor_time_features)\n",
    "        dst_patches_nodes_neighbor_co_occurrence_features = self.projection_layer['neighbor_co_occurrence'](dst_patches_nodes_neighbor_co_occurrence_features)\n",
    "\n",
    "        batch_size = len(src_patches_nodes_neighbor_node_raw_features)\n",
    "        src_num_patches = src_patches_nodes_neighbor_node_raw_features.shape[1]\n",
    "        dst_num_patches = dst_patches_nodes_neighbor_node_raw_features.shape[1]\n",
    "\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, channel_embedding_dim)\n",
    "        patches_nodes_neighbor_node_raw_features = torch.cat([src_patches_nodes_neighbor_node_raw_features, dst_patches_nodes_neighbor_node_raw_features], dim=1)\n",
    "        patches_nodes_edge_raw_features = torch.cat([src_patches_nodes_edge_raw_features, dst_patches_nodes_edge_raw_features], dim=1)\n",
    "        patches_nodes_neighbor_time_features = torch.cat([src_patches_nodes_neighbor_time_features, dst_patches_nodes_neighbor_time_features], dim=1)\n",
    "        patches_nodes_neighbor_co_occurrence_features = torch.cat([src_patches_nodes_neighbor_co_occurrence_features, dst_patches_nodes_neighbor_co_occurrence_features], dim=1)\n",
    "\n",
    "        patches_data = [patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features,\n",
    "                        patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features]\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels, channel_embedding_dim)\n",
    "        patches_data = torch.stack(patches_data, dim=2)\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        patches_data = patches_data.reshape(batch_size, src_num_patches + dst_num_patches, self.num_channels * self.channel_embedding_dim)\n",
    "\n",
    "        # Tensor, shape (batch_size, src_num_patches + dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        for transformer in self.transformers:\n",
    "            patches_data = transformer(patches_data)\n",
    "\n",
    "        # src_patches_data, Tensor, shape (batch_size, src_num_patches, num_channels * channel_embedding_dim)\n",
    "        src_patches_data = patches_data[:, : src_num_patches, :]\n",
    "        # dst_patches_data, Tensor, shape (batch_size, dst_num_patches, num_channels * channel_embedding_dim)\n",
    "        dst_patches_data = patches_data[:, src_num_patches: src_num_patches + dst_num_patches, :]\n",
    "        # src_patches_data, Tensor, shape (batch_size, num_channels * channel_embedding_dim)\n",
    "        src_patches_data = torch.mean(src_patches_data, dim=1)\n",
    "        # dst_patches_data, Tensor, shape (batch_size, num_channels * channel_embedding_dim)\n",
    "        dst_patches_data = torch.mean(dst_patches_data, dim=1)\n",
    "\n",
    "        # Tensor, shape (batch_size, node_feat_dim)\n",
    "        src_node_embeddings = self.output_layer(src_patches_data)\n",
    "        # Tensor, shape (batch_size, node_feat_dim)\n",
    "        dst_node_embeddings = self.output_layer(dst_patches_data)\n",
    "\n",
    "        return src_node_embeddings, dst_node_embeddings\n",
    "\n",
    "    def pad_sequences(self, node_ids: np.ndarray, node_interact_times: np.ndarray, nodes_neighbor_ids_list: list, nodes_edge_ids_list: list,\n",
    "                      nodes_neighbor_times_list: list, patch_size: int = 1, max_input_sequence_length: int = 256):\n",
    "        \"\"\"\n",
    "        pad the sequences for nodes in node_ids\n",
    "        :param node_ids: ndarray, shape (batch_size, )\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :param nodes_neighbor_ids_list: list of ndarrays, each ndarray contains neighbor ids for nodes in node_ids\n",
    "        :param nodes_edge_ids_list: list of ndarrays, each ndarray contains edge ids for nodes in node_ids\n",
    "        :param nodes_neighbor_times_list: list of ndarrays, each ndarray contains neighbor interaction timestamp for nodes in node_ids\n",
    "        :param patch_size: int, patch size\n",
    "        :param max_input_sequence_length: int, maximal number of neighbors for each node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert max_input_sequence_length - 1 > 0, 'Maximal number of neighbors for each node should be greater than 1!'\n",
    "        max_seq_length = 0\n",
    "        # first cut the sequence of nodes whose number of neighbors is more than max_input_sequence_length - 1 (we need to include the target node in the sequence)\n",
    "        for idx in range(len(nodes_neighbor_ids_list)):\n",
    "            assert len(nodes_neighbor_ids_list[idx]) == len(nodes_edge_ids_list[idx]) == len(nodes_neighbor_times_list[idx])\n",
    "            if len(nodes_neighbor_ids_list[idx]) > max_input_sequence_length - 1:\n",
    "                # cut the sequence by taking the most recent max_input_sequence_length interactions\n",
    "                nodes_neighbor_ids_list[idx] = nodes_neighbor_ids_list[idx][-(max_input_sequence_length - 1):]\n",
    "                nodes_edge_ids_list[idx] = nodes_edge_ids_list[idx][-(max_input_sequence_length - 1):]\n",
    "                nodes_neighbor_times_list[idx] = nodes_neighbor_times_list[idx][-(max_input_sequence_length - 1):]\n",
    "            if len(nodes_neighbor_ids_list[idx]) > max_seq_length:\n",
    "                max_seq_length = len(nodes_neighbor_ids_list[idx])\n",
    "\n",
    "        # include the target node itself\n",
    "        max_seq_length += 1\n",
    "        if max_seq_length % patch_size != 0:\n",
    "            max_seq_length += (patch_size - max_seq_length % patch_size)\n",
    "        assert max_seq_length % patch_size  == 0\n",
    "\n",
    "        # pad the sequences\n",
    "        # three ndarrays with shape (batch_size, max_seq_length)\n",
    "        padded_nodes_neighbor_ids = np.zeros((len(node_ids), max_seq_length)).astype(np.long)\n",
    "        padded_nodes_edge_ids = np.zeros((len(node_ids), max_seq_length)).astype(np.long)\n",
    "        padded_nodes_neighbor_times = np.zeros((len(node_ids), max_seq_length)).astype(np.float32)\n",
    "\n",
    "        for idx in range(len(node_ids)):\n",
    "            padded_nodes_neighbor_ids[idx, 0] = node_ids[idx]\n",
    "            padded_nodes_edge_ids[idx, 0] = 0\n",
    "            padded_nodes_neighbor_times[idx, 0] = node_interact_times[idx]\n",
    "\n",
    "            if len(nodes_neighbor_ids_list[idx]) > 0:\n",
    "                padded_nodes_neighbor_ids[idx, 1: len(nodes_neighbor_ids_list[idx]) + 1] = nodes_neighbor_ids_list[idx]\n",
    "                padded_nodes_edge_ids[idx, 1: len(nodes_edge_ids_list[idx]) + 1] = nodes_edge_ids_list[idx]\n",
    "                padded_nodes_neighbor_times[idx, 1: len(nodes_neighbor_times_list[idx]) + 1] = nodes_neighbor_times_list[idx]\n",
    "\n",
    "        # three ndarrays with shape (batch_size, max_seq_length)\n",
    "        return padded_nodes_neighbor_ids, padded_nodes_edge_ids, padded_nodes_neighbor_times\n",
    "\n",
    "    def get_features(self, node_interact_times: np.ndarray, padded_nodes_neighbor_ids: np.ndarray, padded_nodes_edge_ids: np.ndarray,\n",
    "                     padded_nodes_neighbor_times: np.ndarray, time_encoder: TimeEncoder):\n",
    "        \"\"\"\n",
    "        get node, edge and time features\n",
    "        :param node_interact_times: ndarray, shape (batch_size, )\n",
    "        :param padded_nodes_neighbor_ids: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param padded_nodes_edge_ids: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param padded_nodes_neighbor_times: ndarray, shape (batch_size, max_seq_length)\n",
    "        :param time_encoder: TimeEncoder, time encoder\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Tensor, shape (batch_size, max_seq_length, node_feat_dim)\n",
    "        padded_nodes_neighbor_node_raw_features = self.node_raw_features[torch.from_numpy(padded_nodes_neighbor_ids)]\n",
    "        # Tensor, shape (batch_size, max_seq_length, edge_feat_dim)\n",
    "        padded_nodes_edge_raw_features = self.edge_raw_features[torch.from_numpy(padded_nodes_edge_ids)]\n",
    "        # Tensor, shape (batch_size, max_seq_length, time_feat_dim)\n",
    "        padded_nodes_neighbor_time_features = time_encoder(timestamps=torch.from_numpy(node_interact_times[:, np.newaxis] - padded_nodes_neighbor_times).float().to(self.device))\n",
    "\n",
    "        # ndarray, set the time features to all zeros for the padded timestamp\n",
    "        padded_nodes_neighbor_time_features[torch.from_numpy(padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "\n",
    "        return padded_nodes_neighbor_node_raw_features, padded_nodes_edge_raw_features, padded_nodes_neighbor_time_features\n",
    "\n",
    "    def get_patches(self, padded_nodes_neighbor_node_raw_features: torch.Tensor, padded_nodes_edge_raw_features: torch.Tensor,\n",
    "                    padded_nodes_neighbor_time_features: torch.Tensor, padded_nodes_neighbor_co_occurrence_features: torch.Tensor = None, patch_size: int = 1):\n",
    "        \"\"\"\n",
    "        get the sequence of patches for nodes\n",
    "        :param padded_nodes_neighbor_node_raw_features: Tensor, shape (batch_size, max_seq_length, node_feat_dim)\n",
    "        :param padded_nodes_edge_raw_features: Tensor, shape (batch_size, max_seq_length, edge_feat_dim)\n",
    "        :param padded_nodes_neighbor_time_features: Tensor, shape (batch_size, max_seq_length, time_feat_dim)\n",
    "        :param padded_nodes_neighbor_co_occurrence_features: Tensor, shape (batch_size, max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        :param patch_size: int, patch size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert padded_nodes_neighbor_node_raw_features.shape[1] % patch_size == 0\n",
    "        num_patches = padded_nodes_neighbor_node_raw_features.shape[1] // patch_size\n",
    "\n",
    "        # list of Tensors with shape (num_patches, ), each Tensor with shape (batch_size, patch_size, node_feat_dim)\n",
    "        patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features, \\\n",
    "        patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features = [], [], [], []\n",
    "\n",
    "        for patch_id in range(num_patches):\n",
    "            start_idx = patch_id * patch_size\n",
    "            end_idx = patch_id * patch_size + patch_size\n",
    "            patches_nodes_neighbor_node_raw_features.append(padded_nodes_neighbor_node_raw_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_edge_raw_features.append(padded_nodes_edge_raw_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_neighbor_time_features.append(padded_nodes_neighbor_time_features[:, start_idx: end_idx, :])\n",
    "            patches_nodes_neighbor_co_occurrence_features.append(padded_nodes_neighbor_co_occurrence_features[:, start_idx: end_idx, :])\n",
    "\n",
    "        batch_size = len(padded_nodes_neighbor_node_raw_features)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * node_feat_dim)\n",
    "        patches_nodes_neighbor_node_raw_features = torch.stack(patches_nodes_neighbor_node_raw_features, dim=1).reshape(batch_size, num_patches, patch_size * self.node_feat_dim)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * edge_feat_dim)\n",
    "        patches_nodes_edge_raw_features = torch.stack(patches_nodes_edge_raw_features, dim=1).reshape(batch_size, num_patches, patch_size * self.edge_feat_dim)\n",
    "        # Tensor, shape (batch_size, num_patches, patch_size * time_feat_dim)\n",
    "        patches_nodes_neighbor_time_features = torch.stack(patches_nodes_neighbor_time_features, dim=1).reshape(batch_size, num_patches, patch_size * self.time_feat_dim)\n",
    "\n",
    "        patches_nodes_neighbor_co_occurrence_features = torch.stack(patches_nodes_neighbor_co_occurrence_features, dim=1).reshape(batch_size, num_patches, patch_size * self.neighbor_co_occurrence_feat_dim)\n",
    "\n",
    "        return patches_nodes_neighbor_node_raw_features, patches_nodes_edge_raw_features, patches_nodes_neighbor_time_features, patches_nodes_neighbor_co_occurrence_features\n",
    "\n",
    "    def set_neighbor_sampler(self, neighbor_sampler: NeighborSampler):\n",
    "        \"\"\"\n",
    "        set neighbor sampler to neighbor_sampler and reset the random state (for reproducing the results for uniform and time_interval_aware sampling)\n",
    "        :param neighbor_sampler: NeighborSampler, neighbor sampler\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.neighbor_sampler = neighbor_sampler\n",
    "        if self.neighbor_sampler.sample_neighbor_strategy in ['uniform', 'time_interval_aware']:\n",
    "            assert self.neighbor_sampler.seed is not None\n",
    "            self.neighbor_sampler.reset_random_state()\n",
    "\n",
    "\n",
    "class NeighborCooccurrenceEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neighbor_co_occurrence_feat_dim: int, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Neighbor co-occurrence encoder.\n",
    "        :param neighbor_co_occurrence_feat_dim: int, dimension of neighbor co-occurrence features (encodings)\n",
    "        :param device: str, device\n",
    "        \"\"\"\n",
    "        super(NeighborCooccurrenceEncoder, self).__init__()\n",
    "        self.neighbor_co_occurrence_feat_dim = neighbor_co_occurrence_feat_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.neighbor_co_occurrence_encode_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=self.neighbor_co_occurrence_feat_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.neighbor_co_occurrence_feat_dim, out_features=self.neighbor_co_occurrence_feat_dim))\n",
    "\n",
    "    def count_nodes_appearances(self, src_padded_nodes_neighbor_ids: np.ndarray, dst_padded_nodes_neighbor_ids: np.ndarray):\n",
    "        \"\"\"\n",
    "        count the appearances of nodes in the sequences of source and destination nodes\n",
    "        :param src_padded_nodes_neighbor_ids: ndarray, shape (batch_size, src_max_seq_length)\n",
    "        :param dst_padded_nodes_neighbor_ids:: ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # two lists to store the appearances of source and destination nodes\n",
    "        src_padded_nodes_appearances, dst_padded_nodes_appearances = [], []\n",
    "        # src_padded_node_neighbor_ids, ndarray, shape (src_max_seq_length, )\n",
    "        # dst_padded_node_neighbor_ids, ndarray, shape (dst_max_seq_length, )\n",
    "        for src_padded_node_neighbor_ids, dst_padded_node_neighbor_ids in zip(src_padded_nodes_neighbor_ids, dst_padded_nodes_neighbor_ids):\n",
    "\n",
    "            # src_unique_keys, ndarray, shape (num_src_unique_keys, )\n",
    "            # src_inverse_indices, ndarray, shape (src_max_seq_length, )\n",
    "            # src_counts, ndarray, shape (num_src_unique_keys, )\n",
    "            # we can use src_unique_keys[src_inverse_indices] to reconstruct the original input, and use src_counts[src_inverse_indices] to get counts of the original input\n",
    "            src_unique_keys, src_inverse_indices, src_counts = np.unique(src_padded_node_neighbor_ids, return_inverse=True, return_counts=True)\n",
    "            # Tensor, shape (src_max_seq_length, )\n",
    "            src_padded_node_neighbor_counts_in_src = torch.from_numpy(src_counts[src_inverse_indices]).float().to(self.device)\n",
    "            # dictionary, store the mapping relation from unique neighbor id to its appearances for the source node\n",
    "            src_mapping_dict = dict(zip(src_unique_keys, src_counts))\n",
    "\n",
    "            # dst_unique_keys, ndarray, shape (num_dst_unique_keys, )\n",
    "            # dst_inverse_indices, ndarray, shape (dst_max_seq_length, )\n",
    "            # dst_counts, ndarray, shape (num_dst_unique_keys, )\n",
    "            # we can use dst_unique_keys[dst_inverse_indices] to reconstruct the original input, and use dst_counts[dst_inverse_indices] to get counts of the original input\n",
    "            dst_unique_keys, dst_inverse_indices, dst_counts = np.unique(dst_padded_node_neighbor_ids, return_inverse=True, return_counts=True)\n",
    "            # Tensor, shape (dst_max_seq_length, )\n",
    "            dst_padded_node_neighbor_counts_in_dst = torch.from_numpy(dst_counts[dst_inverse_indices]).float().to(self.device)\n",
    "            # dictionary, store the mapping relation from unique neighbor id to its appearances for the destination node\n",
    "            dst_mapping_dict = dict(zip(dst_unique_keys, dst_counts))\n",
    "\n",
    "            # we need to use copy() to avoid the modification of src_padded_node_neighbor_ids\n",
    "            # Tensor, shape (src_max_seq_length, )\n",
    "            src_padded_node_neighbor_counts_in_dst = torch.from_numpy(src_padded_node_neighbor_ids.copy()).apply_(lambda neighbor_id: dst_mapping_dict.get(neighbor_id, 0.0)).float().to(self.device)\n",
    "            # Tensor, shape (src_max_seq_length, 2)\n",
    "            src_padded_nodes_appearances.append(torch.stack([src_padded_node_neighbor_counts_in_src, src_padded_node_neighbor_counts_in_dst], dim=1))\n",
    "\n",
    "            # we need to use copy() to avoid the modification of dst_padded_node_neighbor_ids\n",
    "            # Tensor, shape (dst_max_seq_length, )\n",
    "            dst_padded_node_neighbor_counts_in_src = torch.from_numpy(dst_padded_node_neighbor_ids.copy()).apply_(lambda neighbor_id: src_mapping_dict.get(neighbor_id, 0.0)).float().to(self.device)\n",
    "            # Tensor, shape (dst_max_seq_length, 2)\n",
    "            dst_padded_nodes_appearances.append(torch.stack([dst_padded_node_neighbor_counts_in_src, dst_padded_node_neighbor_counts_in_dst], dim=1))\n",
    "\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances = torch.stack(src_padded_nodes_appearances, dim=0)\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        dst_padded_nodes_appearances = torch.stack(dst_padded_nodes_appearances, dim=0)\n",
    "\n",
    "        # set the appearances of the padded node (with zero index) to zeros\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances[torch.from_numpy(src_padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        dst_padded_nodes_appearances[torch.from_numpy(dst_padded_nodes_neighbor_ids == 0)] = 0.0\n",
    "\n",
    "        return src_padded_nodes_appearances, dst_padded_nodes_appearances\n",
    "\n",
    "    def forward(self, src_padded_nodes_neighbor_ids: np.ndarray, dst_padded_nodes_neighbor_ids: np.ndarray):\n",
    "        \"\"\"\n",
    "        compute the neighbor co-occurrence features of nodes in src_padded_nodes_neighbor_ids and dst_padded_nodes_neighbor_ids\n",
    "        :param src_padded_nodes_neighbor_ids: ndarray, shape (batch_size, src_max_seq_length)\n",
    "        :param dst_padded_nodes_neighbor_ids:: ndarray, shape (batch_size, dst_max_seq_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # src_padded_nodes_appearances, Tensor, shape (batch_size, src_max_seq_length, 2)\n",
    "        # dst_padded_nodes_appearances, Tensor, shape (batch_size, dst_max_seq_length, 2)\n",
    "        src_padded_nodes_appearances, dst_padded_nodes_appearances = self.count_nodes_appearances(src_padded_nodes_neighbor_ids=src_padded_nodes_neighbor_ids,\n",
    "                                                                                                  dst_padded_nodes_neighbor_ids=dst_padded_nodes_neighbor_ids)\n",
    "\n",
    "        # sum the neighbor co-occurrence features in the sequence of source and destination nodes\n",
    "        # Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        src_padded_nodes_neighbor_co_occurrence_features = self.neighbor_co_occurrence_encode_layer(src_padded_nodes_appearances.unsqueeze(dim=-1)).sum(dim=2)\n",
    "        # Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        dst_padded_nodes_neighbor_co_occurrence_features = self.neighbor_co_occurrence_encode_layer(dst_padded_nodes_appearances.unsqueeze(dim=-1)).sum(dim=2)\n",
    "\n",
    "        # src_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, src_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        # dst_padded_nodes_neighbor_co_occurrence_features, Tensor, shape (batch_size, dst_max_seq_length, neighbor_co_occurrence_feat_dim)\n",
    "        return src_padded_nodes_neighbor_co_occurrence_features, dst_padded_nodes_neighbor_co_occurrence_features\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Transformer encoder.\n",
    "        :param attention_dim: int, dimension of the attention vector\n",
    "        :param num_heads: int, number of attention heads\n",
    "        :param dropout: float, dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # use the MultiheadAttention implemented by PyTorch\n",
    "        self.multi_head_attention = MultiheadAttention(embed_dim=attention_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=attention_dim, out_features=4 * attention_dim),\n",
    "            nn.Linear(in_features=4 * attention_dim, out_features=attention_dim)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.LayerNorm(attention_dim),\n",
    "            nn.LayerNorm(attention_dim)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        encode the inputs by Transformer encoder\n",
    "        :param inputs: Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # note that the MultiheadAttention module accept input data with shape (seq_length, batch_size, input_dim), so we need to transpose the input\n",
    "        # Tensor, shape (num_patches, batch_size, self.attention_dim)\n",
    "        transposed_inputs = inputs.transpose(0, 1)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        transposed_inputs = self.norm_layers[0](transposed_inputs)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        hidden_states = self.multi_head_attention(query=transposed_inputs, key=transposed_inputs, value=transposed_inputs)[0].transpose(0, 1)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        outputs = inputs + self.dropout(hidden_states)\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        hidden_states = self.linear_layers[1](self.dropout(F.gelu(self.linear_layers[0](self.norm_layers[1](outputs)))))\n",
    "        # Tensor, shape (batch_size, num_patches, self.attention_dim)\n",
    "        outputs = outputs + self.dropout(hidden_states)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shutil\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.TGAT import TGAT\n",
    "from models.MemoryModel import MemoryModel, compute_src_dst_node_time_shifts\n",
    "from models.CAWN import CAWN\n",
    "from models.TCL import TCL\n",
    "from models.GraphMixer import GraphMixer\n",
    "# from models.DyGFormer import DyGFormer\n",
    "from models.modules import MergeLayer\n",
    "from utils.utils import set_random_seed, convert_to_gpu, get_parameter_sizes, create_optimizer\n",
    "from utils.utils import get_neighbor_sampler, NegativeEdgeSampler\n",
    "from evaluate_models_utils import evaluate_model_link_prediction\n",
    "from utils.metrics import get_link_prediction_metrics\n",
    "from utils.DataLoader import get_idx_data_loader, get_link_prediction_data\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.load_configs import get_link_prediction_args\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, src_node_ids: np.ndarray, dst_node_ids: np.ndarray, node_interact_times: np.ndarray, edge_ids: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        Data object to store the nodes interaction information.\n",
    "        :param src_node_ids: ndarray\n",
    "        :param dst_node_ids: ndarray\n",
    "        :param node_interact_times: ndarray\n",
    "        :param edge_ids: ndarray\n",
    "        :param labels: ndarray\n",
    "        \"\"\"\n",
    "        self.src_node_ids = src_node_ids\n",
    "        self.dst_node_ids = dst_node_ids\n",
    "        self.node_interact_times = node_interact_times\n",
    "        self.edge_ids = edge_ids\n",
    "        self.labels = labels\n",
    "        self.num_interactions = len(src_node_ids)\n",
    "        self.unique_node_ids = set(src_node_ids) | set(dst_node_ids)\n",
    "        self.num_unique_nodes = len(self.unique_node_ids)\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CustomizedDataset(Dataset):\n",
    "    def __init__(self, indices_list: list):\n",
    "        \"\"\"\n",
    "        Customized dataset.\n",
    "        :param indices_list: list, list of indices\n",
    "        \"\"\"\n",
    "        super(CustomizedDataset, self).__init__()\n",
    "\n",
    "        self.indices_list = indices_list\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        get item at the index in self.indices_list\n",
    "        :param idx: int, the index\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.indices_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_list)\n",
    "\n",
    "\n",
    "def get_idx_data_loader(indices_list: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    get data loader that iterates over indices\n",
    "    :param indices_list: list, list of indices\n",
    "    :param batch_size: int, batch size\n",
    "    :param shuffle: boolean, whether to shuffle the data\n",
    "    :return: data_loader, DataLoader\n",
    "    \"\"\"\n",
    "    dataset = CustomizedDataset(indices_list=indices_list)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_snpashot_idx_data_loaders(datasets: dict, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    get data loader that iterates over indices\n",
    "    :param indices_list: list, list of indices\n",
    "    :param batch_size: int, batch size\n",
    "    :param shuffle: boolean, whether to shuffle the data\n",
    "    :return: data_loader, DataLoader\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for ts in datasets.keys():  \n",
    "        dataset = CustomizedDataset(indices_list=list(range(len(datasets[ts]['edges'].src_node_ids))))\n",
    "\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=False)\n",
    "        out[ts] = data_loader\n",
    "    return out\n",
    "\n",
    "\n",
    "# %%\n",
    "def make_data(df):\n",
    "\n",
    "    src_node_ids = df.u.values.astype(np.long)\n",
    "\n",
    "    dst_node_ids = df.i.values.astype(np.long)\n",
    "    node_interact_times = df.ts.values.astype(np.float64)\n",
    "    edge_ids = df.idx.values.astype(np.long)\n",
    "    labels = df.label.values\n",
    "\n",
    "    nodes_set = set(src_node_ids) | set(dst_node_ids)\n",
    "    num_total_unique_node_ids = len(nodes_set)\n",
    "    src_node_ids = df.u.values.astype(np.long)\n",
    "\n",
    "    dst_node_ids = df.i.values.astype(np.long)\n",
    "    node_interact_times = df.ts.values.astype(np.float64)\n",
    "    edge_ids = np.array(list(range(len(df.idx.values)))).astype(np.long)\n",
    "    labels = df.label.values\n",
    "\n",
    "    train_data = Data(src_node_ids=src_node_ids, dst_node_ids=dst_node_ids,\n",
    "                      node_interact_times=node_interact_times,\n",
    "                    edge_ids=edge_ids, labels=labels)\n",
    "    return train_data\n",
    "    \n",
    "\n",
    "\n",
    "def get_dataset_subset(graph_df,percentage):\n",
    "    grouped = graph_df.groupby('ts')\n",
    "\n",
    "    # Define a function to sample each group proportionally\n",
    "\n",
    "\n",
    "    def percentage_sample(group, percentage):\n",
    "        n = int(len(group) * percentage)\n",
    "        return group.sample(n)\n",
    "\n",
    "\n",
    "    # Apply the proportional_sample function to each group\n",
    "    sampled = grouped.apply(percentage_sample, percentage=percentage).reset_index(drop=True)\n",
    "    graph_df =  graph_df[~graph_df.isin(sampled)].dropna()\n",
    "    return sampled,graph_df\n",
    "\n",
    "def make_data_dictionaries(graph_dfs , d, edge_raw_features,node_raw_features ,time_varying_features):\n",
    "    prev_edges = 0\n",
    "\n",
    "    NODE_FEAT_DIM = EDGE_FEAT_DIM = 172\n",
    "    for ts,graph_df in enumerate(graph_dfs.items()):\n",
    "        graph_df = graph_df[1]\n",
    "\n",
    "\n",
    "        new_df = pd.DataFrame(edge_raw_features[prev_edges:prev_edges + len(graph_df)])\n",
    "        new_df.reset_index(drop=True, inplace=True)\n",
    "   \n",
    "        graph_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "       \n",
    "        graph_df = pd.concat([graph_df, new_df], axis=1)\n",
    " \n",
    "\n",
    "        orginal_length = len(graph_df)\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "        if not time_varying_features:\n",
    "            if node_raw_features.shape[1 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features.shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_features[ts] = np.concatenate([node_raw_features, node_zero_padding], axis=1)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if node_raw_features.shape[2 ] < NODE_FEAT_DIM:\n",
    "                node_zero_padding = np.zeros((node_raw_features[ts].shape[0], 172 - node_raw_features.shape[1]))\n",
    "                node_raw_features = np.concatenate([node_raw_features[ts], node_zero_padding], axis=1)\n",
    "\n",
    "        if edge_raw_features.shape[1] < EDGE_FEAT_DIM:\n",
    "                edge_zero_padding = np.zeros((edge_raw_features.shape[0], 172 - edge_raw_features.shape[1]))\n",
    "                edge_raw_features = np.concatenate([edge_raw_features, edge_zero_padding], axis=1)\n",
    "\n",
    "        assert NODE_FEAT_DIM == node_raw_features.shape[1] and EDGE_FEAT_DIM == edge_raw_features.shape[1], \"Unaligned feature dimensions after feature padding!\"\n",
    "   \n",
    "        d[ts]= {'edges': make_data(graph_df), 'node_features' : node_raw_features , 'edge_features':edge_raw_features}\n",
    "        prev_edges += len(graph_df)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def get_link_prediction_data_snapshots(dataset_name: str, val_ratio: float, test_ratio: float):\n",
    "    \"\"\"\n",
    "    generate data for link prediction task (inductive & transductive settings)\n",
    "    :param dataset_name: str, dataset name\n",
    "    :param val_ratio: float, validation data ratio\n",
    "    :param test_ratio: float, test data ratio\n",
    "    :return: node_raw_features, edge_raw_features, (np.ndarray),\n",
    "            full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data, (Data object)\n",
    "    \"\"\"\n",
    "    # Load data and train val test split\n",
    "    graph_df = pd.read_csv('./processed_data/{}/ml_{}.csv'.format(dataset_name, dataset_name))\n",
    "    \n",
    "    edge_raw_features = np.load('./processed_data/{}/ml_{}.npy'.format(dataset_name, dataset_name))\n",
    "    node_raw_features = np.load('./processed_data/{}/ml_{}_node.npy'.format(dataset_name, dataset_name))\n",
    "    cur_edges = 0\n",
    "    full_data_unstacked = make_data(graph_df)\n",
    "    if len(node_raw_features.shape) > 2: \n",
    "        time_varying_features = True\n",
    "    else: \n",
    "        time_varying_features = False\n",
    "  \n",
    "    val_graph_df , train_graph_df = get_dataset_subset(graph_df, val_ratio)\n",
    "\n",
    "    test_graph_df , train_graph_df = get_dataset_subset(train_graph_df, (len(graph_df) * test_ratio ) /len(train_graph_df))\n",
    "    train_data_unstacked = make_data(train_graph_df)\n",
    "    test_data_unstacked = make_data(test_graph_df)\n",
    "    val_data_unstacked = make_data(val_graph_df)\n",
    "\n",
    "    train_graph_dfs = dict(tuple(train_graph_df.groupby('ts')))\n",
    "    val_graph_dfs = dict(tuple(val_graph_df.groupby('ts')))\n",
    "    test_graph_dfs = dict(tuple(test_graph_df.groupby('ts')))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "    train_data = make_data_dictionaries(train_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    val_data = make_data_dictionaries(val_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "    test_data  =make_data_dictionaries(test_graph_dfs,{},edge_raw_features,node_raw_features,time_varying_features)\n",
    "\n",
    "    return full_data_unstacked,train_data_unstacked,train_data,val_data,test_data\n",
    "  \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# get arguments\n",
    "args = get_link_prediction_args(is_evaluation=False)\n",
    "\n",
    "\n",
    "# get data for training, validation and testing\n",
    "full_data_unstacked,train_data_unstacked,train_datas,val_datas,test_datas= \\\n",
    "    get_link_prediction_data_snapshots(dataset_name=args.dataset_name, val_ratio=args.val_ratio, test_ratio=args.test_ratio)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_neighbor_sampler = get_neighbor_sampler(data=train_data_unstacked, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                time_scaling_factor=args.time_scaling_factor, seed=0)\n",
    "\n",
    "\n",
    "full_neighbor_sampler = get_neighbor_sampler(data=full_data_unstacked, sample_neighbor_strategy=args.sample_neighbor_strategy,\n",
    "                                                time_scaling_factor=args.time_scaling_factor, seed=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=train_data_unstacked.src_node_ids, dst_node_ids=train_data_unstacked.dst_node_ids)\n",
    "val_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data_unstacked.src_node_ids, dst_node_ids=full_data_unstacked.dst_node_ids, seed=0)\n",
    "test_neg_edge_sampler = NegativeEdgeSampler(src_node_ids=full_data_unstacked.src_node_ids, dst_node_ids=full_data_unstacked.dst_node_ids, seed=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_datas[0]['edges'].src_node_ids\n",
    "\n",
    "#\n",
    "train_data_loaders = get_snpashot_idx_data_loaders(train_datas, batch_size=args.batch_size, shuffle=False)\n",
    "val_data_loaders = get_snpashot_idx_data_loaders(val_datas, batch_size=args.batch_size, shuffle=False)\n",
    "test_data_loaders = get_snpashot_idx_data_loaders(test_datas,  batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_metric_all_runs, new_node_val_metric_all_runs, test_metric_all_runs, new_node_test_metric_all_runs = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.save_model_name = f'{args.model_name}'\n",
    "\n",
    "# set up logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "os.makedirs(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\", exist_ok=True)\n",
    "# create file handler that logs debug and higher level messages\n",
    "fh = logging.FileHandler(f\"./logs/{args.model_name}/{args.dataset_name}/{args.save_model_name}/{str(time.time())}.log\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.WARNING)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "run_start_time = time.time()\n",
    "\n",
    "\n",
    "logger.info(f'configuration is {args}')\n",
    "\n",
    "\n",
    "node_feature_dim = train_datas[0]['node_features'].shape[1]\n",
    "edge_feature_dim = train_datas[0]['edge_features'].shape[1]\n",
    "\n",
    "dynamic_backbone = DyGFormer(node_raw_features=None, edge_raw_features=None, neighbor_sampler=train_neighbor_sampler,\n",
    "                                    time_feat_dim=args.time_feat_dim, channel_embedding_dim=args.channel_embedding_dim, patch_size=args.patch_size,\n",
    "                                    num_layers=args.num_layers, num_heads=args.num_heads, dropout=args.dropout,\n",
    "                                    max_input_sequence_length=args.max_input_sequence_length, device=args.device,\n",
    "                                    edge_feature_dim = train_datas[0]['edge_features'].shape[1],\n",
    "                                    node_feature_dim = train_datas[0]['node_features'].shape[1])\n",
    "\n",
    "link_predictor = MergeLayer(input_dim1=node_feature_dim, input_dim2=node_feature_dim,\n",
    "                            hidden_dim=node_feature_dim, output_dim=1)\n",
    "model = nn.Sequential(dynamic_backbone, link_predictor)\n",
    "logger.info(f'model -> {model}')\n",
    "logger.info(f'model name: {args.model_name}, #parameters: {get_parameter_sizes(model) * 4} B, '\n",
    "            f'{get_parameter_sizes(model) * 4 / 1024} KB, {get_parameter_sizes(model) * 4 / 1024 / 1024} MB.')\n",
    "\n",
    "optimizer = create_optimizer(model=model, optimizer_name=args.optimizer, learning_rate=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "model = convert_to_gpu(model, device=args.device)\n",
    "\n",
    "save_model_folder = f\"./saved_models/{args.model_name}/{args.dataset_name}/{args.save_model_name}/\"\n",
    "shutil.rmtree(save_model_folder, ignore_errors=True)\n",
    "os.makedirs(save_model_folder, exist_ok=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=args.patience, save_model_folder=save_model_folder,\n",
    "                                save_model_name=args.save_model_name, logger=logger, model_name=args.model_name)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "snapshots = train_data_loaders.keys()\n",
    "for epoch in range(args.num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    model[0].set_neighbor_sampler(train_neighbor_sampler)\n",
    "    for ts in snapshots :\n",
    "        train_idx_data_loader = train_data_loaders[ts]\n",
    "        # store train losses and metrics\n",
    "        train_data = train_datas[ts]['edges']\n",
    "        edge_features = train_datas[ts]['edge_features']\n",
    "        node_features = train_datas[ts]['node_features']\n",
    "        model[0].set_edge_node_features(node_features,edge_features)\n",
    "        train_losses, train_metrics = [], []\n",
    "        train_idx_data_loader_tqdm = tqdm(train_idx_data_loader, ncols=120, position=0, leave=True)\n",
    "        \n",
    "        for batch_idx, train_data_indices in enumerate(train_idx_data_loader_tqdm):\n",
    "            \n",
    "            batch_src_node_ids, batch_dst_node_ids, batch_node_interact_times, batch_edge_ids = \\\n",
    "                train_data.src_node_ids[train_data_indices], train_data.dst_node_ids[train_data_indices], \\\n",
    "                train_data.node_interact_times[train_data_indices], train_data.edge_ids[train_data_indices]\n",
    "\n",
    "            _, batch_neg_dst_node_ids = train_neg_edge_sampler.sample(size=len(batch_src_node_ids))\n",
    "            batch_neg_src_node_ids = batch_src_node_ids\n",
    "\n",
    "            \n",
    "            \n",
    "            # get temporal embedding of source and destination nodes\n",
    "            # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "            batch_src_node_embeddings, batch_dst_node_embeddings = \\\n",
    "                model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,\n",
    "                                                                    dst_node_ids=batch_dst_node_ids,\n",
    "                                                                    node_interact_times=batch_node_interact_times)\n",
    "\n",
    "            # get temporal embedding of negative source and negative destination nodes\n",
    "            # two Tensors, with shape (batch_size, node_feat_dim)\n",
    "            batch_neg_src_node_embeddings, batch_neg_dst_node_embeddings = \\\n",
    "                model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,\n",
    "                                                                    dst_node_ids=batch_neg_dst_node_ids,\n",
    "                                                                    node_interact_times=batch_node_interact_times)\n",
    "            \n",
    "            \n",
    "            positive_probabilities = model[1](input_1=batch_src_node_embeddings, input_2=batch_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "            negative_probabilities = model[1](input_1=batch_neg_src_node_embeddings, input_2=batch_neg_dst_node_embeddings).squeeze(dim=-1).sigmoid()\n",
    "\n",
    "            predicts = torch.cat([positive_probabilities, negative_probabilities], dim=0)\n",
    "            labels = torch.cat([torch.ones_like(positive_probabilities), torch.zeros_like(negative_probabilities)], dim=0)\n",
    "            loss = loss_func(input=predicts, target=labels)\n",
    "            # try:\n",
    "            #     loss = loss_func(input=predicts, target=labels)\n",
    "            # except Exception:\n",
    "\n",
    "            #     print(predicts,labels)\n",
    "            #     exit(0)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            train_metrics.append(get_link_prediction_metrics(predicts=predicts, labels=labels))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_idx_data_loader_tqdm.set_description(f'Epoch: {epoch + 1}, at timestamp : {ts} train for the {batch_idx + 1}-th batch, train loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TGN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dd3e09ad2d66205a5dc358440236d734a99104d202f72a697bec28977974f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
